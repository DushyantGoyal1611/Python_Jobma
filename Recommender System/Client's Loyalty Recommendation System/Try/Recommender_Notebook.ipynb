{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5647f89-0a87-4c57-a844-257f4fc516e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data Encoding and Scaling\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pipeline\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from functools import lru_cache\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a494ec9-7676-4e91-8995-e145c9c90ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8180be-3159-42eb-8802-e94dedbedfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660949a3-f1c2-4c1d-8f61-784c9fe6760b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading .env file into my python code\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66b254e-68cf-40bb-b130-e7ec340b04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    print('creating connection with DB')\n",
    "    user = os.getenv(\"DB_USER\")\n",
    "    raw_password = os.getenv(\"DB_PASSWORD\")\n",
    "    password = quote_plus(raw_password)\n",
    "    host = os.getenv(\"DB_HOST\")\n",
    "    port = os.getenv(\"DB_PORT\")\n",
    "    db = os.getenv(\"DB_NAME\")\n",
    "\n",
    "    # Credentials of mySQL connection\n",
    "    connection_string = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print('connection created successfully')\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee7c29e-c096-4e0c-8f56-532262cf77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(engine,catcher_only=False):\n",
    "    print('creating DFs=============')\n",
    "    if catcher_only:\n",
    "        print('Catcher DF is requested')\n",
    "        catcher_df = pd.read_sql('Select jobma_catcher_id, is_premium, jobma_catcher_parent, jobma_verified, subscription_status, company_size FROM jobma_catcher', con=engine) \n",
    "        return catcher_df\n",
    "    print(\"Wallet DF\")\n",
    "    wallet_df = pd.read_sql('Select catcher_id, is_unlimited FROM wallet', con=engine)\n",
    "    print(\"Subscription DF\")\n",
    "    subscription_df = pd.read_sql('Select catcher_id, currency, subscription_amount FROM subscription_history', con=engine)\n",
    "    print(\"Invitation DF\")\n",
    "    invitation_df = pd.read_sql('Select jobma_catcher_id, jobma_interview_mode, jobma_interview_status FROM jobma_pitcher_invitations', con=engine)\n",
    "    print(\"Job posting DF\")\n",
    "    job_posting_df = pd.read_sql('Select jobma_catcher_id FROM jobma_employer_job_posting', con=engine)\n",
    "    print(\"kit DF\")\n",
    "    kit_df = pd.read_sql('Select catcher_id FROM job_assessment_kit', con=engine)\n",
    "    print('Login DF')\n",
    "    login_df = pd.read_sql('Select jobma_role_id, jobma_user_id, jobma_last_login FROM jobma_login',con=engine)\n",
    "    # Closing the Connection\n",
    "    engine.dispose()\n",
    "    return wallet_df,subscription_df,invitation_df,job_posting_df,kit_df,login_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088775a-298f-4ea7-a887-f8ffee369afc",
   "metadata": {},
   "source": [
    "# Specific Methods\n",
    "\n",
    "**To Fetch Columns from different tables and fitting those functions into Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a673c2-3b1c-4164-a0ba-5075a5b1ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcher_df\n",
    "\n",
    "def fetching_catcher_df(catcher_df):\n",
    "    print(\"Processing catcher DF\")\n",
    "    catcher_df = catcher_df[['jobma_catcher_id', 'is_premium', 'jobma_catcher_parent', 'jobma_verified', 'subscription_status', 'company_size']]\n",
    "    catcher_df['jobma_verified'] = catcher_df['jobma_verified'].replace({'0':0, '1':1})\n",
    "    catcher_df.drop(catcher_df[catcher_df['is_premium'] == ''].index, inplace=True)\n",
    "    catcher_df['is_premium'] = catcher_df['is_premium'].replace({'0':0, '1':1})\n",
    "    catcher_df['subscription_status'] = catcher_df['subscription_status'].replace({'0':0, '1':1, '2':0})\n",
    "\n",
    "    return catcher_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62244ab9-feed-491d-94d4-5a3db6cd92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_df\n",
    "\n",
    "def fetching_wallet_df(wallet_df):\n",
    "    print(\"Processing wallet DF\")\n",
    "    wallet_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    wallet_df = wallet_df[['jobma_catcher_id', 'is_unlimited']]\n",
    "    wallet_df['is_unlimited'] = wallet_df['is_unlimited'].replace({'0':0, '1':1})\n",
    "    wallet_df.drop(wallet_df[wallet_df['is_unlimited'] == ''].index, inplace=True)\n",
    "    wallet_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return wallet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df263e4-9d4c-4bc6-9cca-77ca10b33926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_df\n",
    "\n",
    "def fetching_subscription_df(subscription_df):\n",
    "    print(\"Processing subscription DF\")\n",
    "    subscription_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    subscription_df.loc[subscription_df['currency'] == '1', 'subscription_amount'] /= 85.23\n",
    "    subscription_df = subscription_df.groupby('jobma_catcher_id').agg(\n",
    "        subscription_amount_in_dollars = ('subscription_amount', 'sum'),\n",
    "        number_of_subscriptions = ('subscription_amount', 'count'),\n",
    "    ).reset_index()\n",
    "    subscription_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return subscription_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4338415-997f-4948-bd30-a32ea18d0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login_df\n",
    "\n",
    "def fetching_login_df(login_df):\n",
    "    print(\"Processing login DF\")\n",
    "    login_df = login_df[login_df['jobma_role_id'] == 3].copy()\n",
    "    login_df.rename(columns={'jobma_user_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    # Calculating Number of Gaps between last login and today\n",
    "    login_df['jobma_last_login'] = pd.to_datetime(login_df['jobma_last_login'], errors='coerce')\n",
    "    login_df['days_since_last_login'] = (pd.Timestamp('today') - login_df['jobma_last_login']).dt.days\n",
    "    login_df['days_since_last_login'].fillna(5, inplace=True)\n",
    "    login_df['days_since_last_login'] = login_df['days_since_last_login'].astype(int)\n",
    "\n",
    "    # Binning\n",
    "    bins = [-1,7,30,90,180,365,float('inf')]\n",
    "    labels = ['Less than 1 Week', '1-4 Weeks', '1-3 Months', '3-6 Months', '6-12 Months', 'More than 1 Year']\n",
    "    login_df['days_since_last_login'] = pd.cut(login_df['days_since_last_login'], bins=bins, labels=labels)\n",
    "    login_df = login_df[['jobma_catcher_id', 'days_since_last_login']]\n",
    "\n",
    "    return login_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4ced3a-819b-4922-af55-70f67375a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_features(invitation_df, job_posting_df, kit_df):\n",
    "    print(\"Fetching features\")\n",
    "    for df in [invitation_df, job_posting_df, kit_df]:\n",
    "        if 'catcher_id' in df.columns:\n",
    "            df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    job_posting_df['job_posted'] = job_posting_df['jobma_catcher_id'].map(job_posting_df['jobma_catcher_id'].value_counts())\n",
    "    kit_df['number_of_kits'] = kit_df['jobma_catcher_id'].map(kit_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df['number_of_invitations'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df = invitation_df[invitation_df['jobma_interview_mode'].isin(['1', '2'])].copy()\n",
    "    interview_counts = invitation_df.groupby(['jobma_catcher_id', 'jobma_interview_mode']).size().unstack(fill_value=0)\n",
    "    interview_counts = interview_counts.rename(columns={'1': 'number_of_recorded_interviews', '2': 'number_of_live_interviews'})\n",
    "    invitation_df = invitation_df.merge(interview_counts, on='jobma_catcher_id', how='left')\n",
    "    invitation_df.drop(invitation_df[invitation_df['jobma_interview_status'] == ''].index, inplace=True)\n",
    "    invitation_df.drop(invitation_df[invitation_df['jobma_interview_status'] == '0'].index, inplace=True)\n",
    "    invitation_df['jobma_interview_status'] = invitation_df['jobma_interview_status'].astype(int)\n",
    "    invitation_df = invitation_df.drop_duplicates()\n",
    "\n",
    "    # invitation_df = invitation_df[['jobma_catcher_id', 'number_of_invitations']].drop_duplicates()\n",
    "    job_posting_df = job_posting_df[['jobma_catcher_id', 'job_posted']].drop_duplicates()\n",
    "    kit_df = kit_df[['jobma_catcher_id', 'number_of_kits']].drop_duplicates()\n",
    "\n",
    "    return invitation_df, job_posting_df, kit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c104ba-cf89-44ae-b5cf-c2c85a9a0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df(catcher_df, wallet_df, subscription_df, invitation_df, job_posting_df, kit_df, recorded_interview_df, live_interview_df, login_df):\n",
    "    print(\"Merging DFs\")\n",
    "    final_df = catcher_df.copy()\n",
    "\n",
    "    # Left join each table one by one\n",
    "    final_df = final_df.merge(wallet_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(subscription_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(invitation_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(job_posting_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(kit_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(recorded_interview_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(live_interview_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(login_df, on='jobma_catcher_id', how='left')\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # For Total Sub\n",
    "    sub_counts = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent').size()\n",
    "    final_df['total_sub'] = final_df['jobma_catcher_id'].map(sub_counts).fillna(0).astype(int)\n",
    "\n",
    "    # For Kits\n",
    "    sub_kits_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_kits'].sum()\n",
    "    # final_df.loc[final_df['jobma_catcher_id'].isin(sub_kits_sum.index), 'number_of_kits'] += final_df['jobma_catcher_id'].map(sub_kits_sum)\n",
    "    kits_mapped = final_df['jobma_catcher_id'].map(sub_kits_sum).fillna(0)\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].fillna(0) + kits_mapped\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].astype(int)\n",
    "\n",
    "    \n",
    "    # For Invitations\n",
    "    sub_invitations_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_invitations'].sum()\n",
    "    # final_df.loc[final_df['jobma_catcher_id'].isin(sub_kits_sum.index), 'number_of_invitations'] += final_df['jobma_catcher_id'].map(sub_invitations_sum)\n",
    "    invitations_mapped = final_df['jobma_catcher_id'].map(sub_invitations_sum).fillna(0)\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].fillna(0) + invitations_mapped\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].astype(int)\n",
    "    \n",
    "    # For Job Posted\n",
    "    sub_job_posted_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['job_posted'].sum()\n",
    "    # final_df.loc[final_df['jobma_catcher_id'].isin(sub_job_posted_sum.index), 'job_posted'] += final_df['jobma_catcher_id'].map(sub_job_posted_sum)\n",
    "    job_posted_mapped = final_df['jobma_catcher_id'].map(sub_job_posted_sum).fillna(0)\n",
    "    final_df['job_posted'] = final_df['job_posted'].fillna(0) + job_posted_mapped\n",
    "    final_df['job_posted'] = final_df['job_posted'].astype(int)\n",
    "\n",
    "    # For Recorded Interviews\n",
    "    sub_recorded_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_recorded_interviews'].sum()\n",
    "    # final_df.loc[final_df['jobma_catcher_id'].isin(sub_recorded_sum.index), 'number_of_recorded_interviews'] += final_df['jobma_catcher_id'].map(sub_recorded_sum)\n",
    "    recorded_mapped = final_df['jobma_catcher_id'].map(sub_recorded_sum).fillna(0)\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].fillna(0) + recorded_mapped\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].astype(int)\n",
    "\n",
    "    # For Live Interviews\n",
    "    sub_live_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_live_interviews'].sum()\n",
    "    # final_df.loc[final_df['jobma_catcher_id'].isin(sub_live_sum.index), 'number_of_live_interviews'] += final_df['jobma_catcher_id'].map(sub_live_sum)\n",
    "    live_mapped = final_df['jobma_catcher_id'].map(sub_live_sum).fillna(0)\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].fillna(0) + live_mapped\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].astype(int)\n",
    "\n",
    "    # For Minimum Login Days\n",
    "    login_order = {\n",
    "        'Less than 1 Week':0,\n",
    "        '1-4 Weeks':1,\n",
    "        '1-3 Months':2,\n",
    "        '3-6 Months':3,\n",
    "        '6-12 Months':4,\n",
    "        'More than 1 Year':5\n",
    "    }\n",
    "\n",
    "    final_df['days_since_last_login'] = final_df['days_since_last_login'].map(login_order).fillna(5).astype(int)\n",
    "    sub_min_login = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['days_since_last_login'].min()\n",
    "    final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'days_since_last_login'] = final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'jobma_catcher_id'].map(sub_min_login)\n",
    "\n",
    "    verified_df = final_df[final_df['jobma_verified'] == 1].copy()\n",
    "    df = verified_df[verified_df['jobma_catcher_parent'] == 0].copy()\n",
    "    df.drop(['jobma_catcher_parent', 'jobma_verified'], axis=1, inplace=True)\n",
    "    \n",
    "    compare_df = df.copy()\n",
    "    df.drop('jobma_catcher_id', axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Final merged df shape is {df.shape}\")\n",
    "\n",
    "    return df, compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b2801d-f220-4bbe-9c09-15837ca7d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function is to fill all missing values\n",
    "\n",
    "def fill_missing_values(final_df):\n",
    "    final_df = final_df.copy()\n",
    "    fill_values = {\n",
    "        'is_premium': 0,\n",
    "        'subscription_status': 0,\n",
    "        'company_size': '1-25',\n",
    "        'is_unlimited': 0,\n",
    "        'subscription_amount_in_dollars': 0,\n",
    "        'number_of_subscriptions': 0,\n",
    "        'jobma_interview_status': 1,\n",
    "        'number_of_invitations': 0,\n",
    "        'number_of_recorded_interviews': 0,\n",
    "        'number_of_live_interviews': 0,\n",
    "        'job_posted': 0,\n",
    "        'number_of_kits': 0,\n",
    "        'days_since_last_login': 5,\n",
    "        'total_sub': 0,\n",
    "    }\n",
    "    \n",
    "    return final_df.fillna(fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7537f3-7d52-4519-8ff5-03b5da8b1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Encoding \n",
    "\n",
    "def ordinal_encoder(df):\n",
    "    # ordinal_col = ['company_size', 'days_since_last_login']\n",
    "    ordinal_col = ['company_size']\n",
    "    company_size_order = ['1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "    # login_days_order = ['Less than 1 Week', '1-4 Weeks', '1-3 Months', '3-6 Months', '6-12 Months', 'More than 1 Year']\n",
    "\n",
    "    # total_order = [company_size_order, login_days_order]\n",
    "    ordinal = OrdinalEncoder(categories=[company_size_order])\n",
    "\n",
    "    encoded = ordinal.fit_transform(df[ordinal_col].astype(str))\n",
    "    # encoded += 1\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f' {col}_ord' for col in ordinal_col], index=df.index)\n",
    "\n",
    "    df.drop(columns=ordinal_col, inplace=True)\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba4d4c5-4b07-4cbb-97b2-3063aaf1114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation \n",
    "\n",
    "def log_transform(df):\n",
    "    log_cols = [\n",
    "        'total_sub',\n",
    "        'subscription_amount_in_dollars',\n",
    "        'number_of_subscriptions',\n",
    "        'number_of_invitations',\n",
    "        'job_posted',\n",
    "        'number_of_kits',\n",
    "        'number_of_recorded_interviews',\n",
    "        'number_of_live_interviews',\n",
    "        'days_since_last_login'\n",
    "    ]\n",
    "\n",
    "    df = df.copy()\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            # fill NaNs\n",
    "            df[col] = df[col].fillna(0)\n",
    "            # if a number is less than zero, turn it into zero\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "            # safe log1p\n",
    "            df[col] = np.log1p(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7606cc4-020a-44af-a40e-ab08f868c4dc",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e46d91f-b5b6-47bc-b331-3d4203280edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        engine = create_connection()\n",
    "        wallet_df,subscription_df,invitation_df,job_posting_df,kit_df,recorded_interview_df,live_interview_df,login_df = create_df(engine)\n",
    "        self.wallet_df = wallet_df\n",
    "        self.subscription_df = subscription_df\n",
    "        self.invitation_df = invitation_df\n",
    "        self.job_posting_df = job_posting_df\n",
    "        self.kit_df = kit_df\n",
    "        self.recorded_interview_df = recorded_interview_df\n",
    "        self.live_interview_df = live_interview_df\n",
    "        self.login_df = login_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, catcher_df):\n",
    "        catcher = fetching_catcher_df(catcher_df)\n",
    "        wallet = fetching_wallet_df(self.wallet_df)\n",
    "        subscription = fetching_subscription_df(self.subscription_df)\n",
    "        login = fetching_login_df(self.login_df)\n",
    "        invitation, job_posting, kit, recorded, live = fetching_features(\n",
    "            self.invitation_df,\n",
    "            self.job_posting_df,\n",
    "            self.kit_df,\n",
    "            self.recorded_interview_df,\n",
    "            self.live_interview_df\n",
    "        )\n",
    "        final_df, compare_df = merging_df(catcher, wallet, subscription, invitation, job_posting, kit, recorded, live, login)\n",
    "        self.compare_df_ = fill_missing_values(compare_df)\n",
    "        \n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf307a4-69a6-4ff6-a050-d3e7999deb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = fill_missing_values(X)\n",
    "        X = ordinal_encoder(X)\n",
    "        X = log_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bef4ac5-70c7-41d6-9ef8-2c766c5be9e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wallet_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Pipelines\u001b[39;00m\n\u001b[0;32m      3\u001b[0m merge_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m----> 4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge\u001b[39m\u001b[38;5;124m'\u001b[39m, MergingTransformer(wallet_df\u001b[38;5;241m=\u001b[39mwallet_df,\n\u001b[0;32m      5\u001b[0m                                  subscription_df\u001b[38;5;241m=\u001b[39msubscription_df,\n\u001b[0;32m      6\u001b[0m                                  login_df\u001b[38;5;241m=\u001b[39mlogin_df,\n\u001b[0;32m      7\u001b[0m                                  invitation_df\u001b[38;5;241m=\u001b[39minvitation_df,\n\u001b[0;32m      8\u001b[0m                                  job_posting_df\u001b[38;5;241m=\u001b[39mjob_posting_df,\n\u001b[0;32m      9\u001b[0m                                  kit_df\u001b[38;5;241m=\u001b[39mkit_df,\n\u001b[0;32m     10\u001b[0m                                  recorded_interview_df\u001b[38;5;241m=\u001b[39mrecorded_interview_df,\n\u001b[0;32m     11\u001b[0m                                  live_interview_df\u001b[38;5;241m=\u001b[39mlive_interview_df))\n\u001b[0;32m     12\u001b[0m ])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Step 2: Preprocess merged data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m preprocess_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     16\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m, PreprocessingTransformer()),\n\u001b[0;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[0;32m     18\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)))\n\u001b[0;32m     19\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wallet_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Pipelines\n",
    "\n",
    "# merge_pipeline = Pipeline([\n",
    "#     ('merge', MergingTransformer(wallet_df=wallet_df,\n",
    "#                                  subscription_df=subscription_df,\n",
    "#                                  login_df=login_df,\n",
    "#                                  invitation_df=invitation_df,\n",
    "#                                  job_posting_df=job_posting_df,\n",
    "#                                  kit_df=kit_df,\n",
    "#                                  recorded_interview_df=recorded_interview_df,\n",
    "#                                  live_interview_df=live_interview_df))\n",
    "# ])\n",
    "\n",
    "# Step 2: Preprocess merged data\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', PreprocessingTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('to_tensor', FunctionTransformer(lambda x: torch.tensor(x, dtype=torch.float32)))\n",
    "])\n",
    "\n",
    "# full_pipeline = Pipeline([\n",
    "#     ('merge_pipeline', merge_pipeline),\n",
    "#     ('preprocess_pipeline', preprocess_pipeline)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce59e0e-df9f-4980-ae77-56eaf527958e",
   "metadata": {},
   "source": [
    "# Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fa81d-a06f-438c-b727-ed5977242ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = full_pipeline.fit_transform(catcher_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6311d2-da7a-46a0-a651-0305fb8d9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dea18-dcd5-4bbb-9481-a13f88327e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9604-0fb5-4244-8abb-b9e1402772af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe851c-0938-4c59-ac10-babbf3fad5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5ecfb-165e-445f-b840-84d53988d7a2",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fad8d-c965-4a2e-aa6a-dd30d093a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a44b4-cd2b-4408-812a-08a1f5c3da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = CustomDataset(X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141c51a-da71-45b7-afe1-b35e370fe7db",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb42c6-dd3f-4a6a-b343-88a645d6b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e8e5c-8cb1-4f11-9675-37cabb43faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(X_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f776a-08fe-4730-96ae-bc80f05ac8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26fdd18-4a36-4296-87d6-445691e39ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2547d-5a8c-4d37-8da5-ad20241ee780",
   "metadata": {},
   "source": [
    "# Define a Model (AutoEncoder in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256a10f-3e80-4a41-8c9f-8001af4e277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # bottleneck\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c09e85-5942-48ce-8251-2970969163c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Model \n",
    "\n",
    "input_shape = X_df.shape[1]\n",
    "model_1 = AutoEncoder(input_shape)\n",
    "model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70f9c6-b768-4e6c-884e-92946fd9b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Parameters \n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "patience = 5\n",
    "delta = 1e-4\n",
    "\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "training_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dd4ec-ccf3-4e31-858e-0f12b5cf38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function, Optimizers and LR Scheduler \n",
    "\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.AdamW(model_1.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8631b-920f-4ad1-aae0-723c5c167c8c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6093b9-9043-455f-8798-51c868f736f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        encoded, decoded = model_1(batch)\n",
    "        loss = loss_function(decoded, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    training_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{epochs} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if avg_loss < best_loss - delta:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Print current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Learning Rate: {param_group['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77c051-693b-4914-9833-5f81c2308d92",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f55eef-0300-4a86-8ccc-baa10f42e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder, _ = model_1(X_tensor)\n",
    "\n",
    "latent_np = encoder.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b78fde-9118-4e04-a39c-b3048f7f8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbf601-d240-47fc-bd25-16ce48569376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(latent_np)\n",
    "\n",
    "    wcss.append(kmeans.inertia_) # Inertia = WCSS\n",
    "    silhouette_scores.append(silhouette_score(latent_np, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd570929-5171-4c05-9525-ae46ab305b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Rule to decide the number of clusters \n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, wcss, marker='o')\n",
    "plt.title('Elbow Method (WCSS vs K)')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "\n",
    "# Step 5: Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, marker='o', color='green')\n",
    "plt.title('Silhouette Score vs K')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674c9bb-a7d5-4019-bf1b-2d9eeb8a5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising KMeans\n",
    "kmeans = KMeans(n_clusters=4, n_init=50, init='k-means++', random_state=42)\n",
    "cluster_ids = kmeans.fit_predict(latent_np)\n",
    "cluster_centers = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a5bd0-9ddf-423d-a72c-c48fba5cc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_2d = PCA(n_components=2).fit_transform(latent_np)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=cluster_ids, cmap='tab10', s=10)\n",
    "plt.title(\"KMeans clusters on latent space (Before DEC)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef872d4-4f6a-47b4-aa69-357a7a3527a5",
   "metadata": {},
   "source": [
    "## Test it on User's Preferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc86757-bb59-44f8-91c3-88e9ca70e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde5b45-b77e-4ccf-9ba6-06fcc94ba019",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = merge_pipeline.named_steps['merge'].compare_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f90251-e12a-480c-bd24-fc01f6b27fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b8889-ce49-4a66-bf99-100f5470bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35ea6-ca73-4396-89e2-8807ee2c1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_good = {'is_premium':1,\n",
    "                 'subscription_status':1,\n",
    "                 'company_size':'More than 1000', \n",
    "                 'is_unlimited':1,\n",
    "                 'subscription_amount_in_dollars': 10000.00,\n",
    "                 'number_of_subscriptions':2,\n",
    "                 'number_of_invitations':25,\n",
    "                 'job_posted':10,\n",
    "                 'number_of_kits':7,\n",
    "                 'number_of_recorded_interviews':18,\n",
    "                'number_of_live_interviews':15,\n",
    "                'days_since_last_login':5,\n",
    "                'total_sub':3,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da2da7-5667-4278-b052-f1df8a4e2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_input, model, latent_embeddings, compare_df, pipeline, top_k=5):\n",
    "    # Transform the user input using the pipeline\n",
    "    user_df = pd.DataFrame([user_input])\n",
    "    user_input_tensor = pipeline.transform(user_df)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the user embedding from the model\n",
    "        user_embedding, _ = model(user_input_tensor)\n",
    "\n",
    "        # Normalize the user embedding\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "\n",
    "        # Convert latent embeddings to tensor and normalize\n",
    "        latent_embeddings_tensor = torch.tensor(latent_embeddings, dtype=torch.float32)\n",
    "        latent_embeddings_tensor = F.normalize(latent_embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarities = F.cosine_similarity(user_embedding, latent_embeddings_tensor, dim=1)\n",
    "\n",
    "        # Get top K most similar indices\n",
    "        top_indices = similarities.topk(top_k).indices.cpu().numpy()\n",
    "\n",
    "        # Get the top K recommendations from the compare_df\n",
    "        recommended = compare_df.iloc[top_indices].copy()\n",
    "        recommended['similarity'] = similarities[top_indices].cpu().numpy()\n",
    "\n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e94f17-04e5-46cb-9dbd-b8493bec4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = recommend(user_pref_good, model_1, latent_np, compare_df, preprocess_pipeline, top_k=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e27d7-2b6f-47fa-8d29-3ef544c99529",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_test = {'is_premium':1,\n",
    "                 'subscription_status':1,\n",
    "                 'company_size':'1-25',\n",
    "                 'is_unlimited':1,\n",
    "                 'subscription_amount_in_dollars': 125.0,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':18,\n",
    "                 'job_posted':3,\n",
    "                 'number_of_kits':3,\n",
    "                 'number_of_recorded_interviews':3,\n",
    "                'number_of_live_interviews':1,\n",
    "                'days_since_last_login':4,\n",
    "                'total_sub':2\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa0d7f-e0eb-4dcd-b533-da2c1e391d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = recommend(user_pref_test, model_1, latent_np, compare_df, preprocess_pipeline, top_k=10)\n",
    "result1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
