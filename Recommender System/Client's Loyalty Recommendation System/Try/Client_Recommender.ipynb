{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f7edda-c240-4f29-9c57-fe5e9b144bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data Encoding and Scaling\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92ccb96-f858-4600-9f68-b0f9f784e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db8d71d-3686-4446-b3df-69a80d0aa868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading .env file into my python code\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccaf725-a5f6-4790-82ac-d3fa685c0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    print('creating connection with DB')\n",
    "    user = os.getenv(\"DB_USER\")\n",
    "    raw_password = os.getenv(\"DB_PASSWORD\")\n",
    "    password = quote_plus(raw_password)\n",
    "    host = os.getenv(\"DB_HOST\")\n",
    "    port = os.getenv(\"DB_PORT\")\n",
    "    db = os.getenv(\"DB_NAME\")\n",
    "\n",
    "    # Credentials of mySQL connection\n",
    "    connection_string = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print('connection created successfully')\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697791f2-b367-430e-9357-1dc380ffc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(engine,catcher_only=False):\n",
    "    print('creating DFs=============')\n",
    "    if catcher_only:\n",
    "        print('Catcher DF is requested')\n",
    "        catcher_df = pd.read_sql('Select jobma_catcher_id, is_premium, jobma_catcher_parent, jobma_verified, subscription_status, company_size FROM jobma_catcher', con=engine) \n",
    "        return catcher_df\n",
    "    print(\"Wallet DF\")\n",
    "    wallet_df = pd.read_sql('Select catcher_id, is_unlimited FROM wallet', con=engine)\n",
    "    print(\"Subscription DF\")\n",
    "    subscription_df = pd.read_sql('Select catcher_id, currency, subscription_amount FROM subscription_history', con=engine)\n",
    "    print(\"Invitation DF\")\n",
    "    invitation_df = pd.read_sql('Select jobma_catcher_id, jobma_interview_mode, jobma_interview_status FROM jobma_pitcher_invitations', con=engine)\n",
    "    print(\"Job posting DF\")\n",
    "    job_posting_df = pd.read_sql('Select jobma_catcher_id FROM jobma_employer_job_posting', con=engine)\n",
    "    print(\"kit DF\")\n",
    "    kit_df = pd.read_sql('Select catcher_id FROM job_assessment_kit', con=engine)\n",
    "    print('Login DF')\n",
    "    login_df = pd.read_sql('Select jobma_role_id, jobma_user_id, jobma_last_login FROM jobma_login',con=engine)\n",
    "    # Closing the Connection\n",
    "    engine.dispose()\n",
    "    return wallet_df,subscription_df,invitation_df,job_posting_df,kit_df,login_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffd9ae-a8b3-4496-9070-eb5e64fa6d7a",
   "metadata": {},
   "source": [
    "# Specific Methods\n",
    "\n",
    "**To Fetch Columns from different tables and fitting those functions into Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09161e27-46da-4c2e-99f1-a48a74aa8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcher_df\n",
    "\n",
    "def fetching_catcher_df(catcher_df):\n",
    "    print(\"Processing catcher DF\")\n",
    "    catcher_df['jobma_verified'] = catcher_df['jobma_verified'].replace({'0':0, '1':1})\n",
    "    catcher_df.drop(catcher_df[catcher_df['is_premium'] == ''].index, inplace=True)\n",
    "    catcher_df['is_premium'] = catcher_df['is_premium'].replace({'0':0, '1':1})\n",
    "    catcher_df['subscription_status'] = catcher_df['subscription_status'].replace({'0':0, '1':1, '2':0})\n",
    "\n",
    "    return catcher_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658cf34a-8580-4fd0-854a-ad4ec8aa5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_df\n",
    "\n",
    "def fetching_wallet_df(wallet_df):\n",
    "    print(\"Processing wallet DF\")\n",
    "    wallet_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    wallet_df['is_unlimited'] = wallet_df['is_unlimited'].replace({'0':0, '1':1})\n",
    "    wallet_df = wallet_df[wallet_df['is_unlimited'] != '']\n",
    "    wallet_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return wallet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ae4bef-6e45-46b9-9a9d-5dd5aad3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_df\n",
    "\n",
    "def fetching_subscription_df(subscription_df):\n",
    "    print(\"Processing subscription DF\")\n",
    "    subscription_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    subscription_df.loc[subscription_df['currency'] == '1', 'subscription_amount'] /= 85.23\n",
    "    subscription_df = subscription_df.groupby('jobma_catcher_id').agg(\n",
    "        subscription_amount_in_dollars = ('subscription_amount', 'sum'),\n",
    "        number_of_subscriptions = ('subscription_amount', 'count'),\n",
    "    ).reset_index()\n",
    "    subscription_df.drop_duplicates(inplace=True)\n",
    "    return subscription_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611c17cc-9930-4585-9bb6-62fcbfecf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login_df\n",
    "\n",
    "def fetching_login_df(login_df):\n",
    "    print(\"Processing login DF\")\n",
    "    login_df = login_df[login_df['jobma_role_id'] == 3].copy()\n",
    "    login_df.rename(columns={'jobma_user_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    # Calculating Number of Gaps between last login and today\n",
    "    login_df['jobma_last_login'] = pd.to_datetime(login_df['jobma_last_login'], errors='coerce')\n",
    "    login_df['activity_duration'] = (pd.Timestamp('today') - login_df['jobma_last_login']).dt.days\n",
    "    login_df['activity_duration'].fillna(5, inplace=True)\n",
    "    login_df['activity_duration'] = login_df['activity_duration'].astype(int)\n",
    "\n",
    "    # Binning\n",
    "    bins = [-1,7,30,90,180,365,float('inf')]\n",
    "    labels = ['Less than 1 Week', '1-4 Weeks', '1-3 Months', '3-6 Months', '6-12 Months', 'More than 1 Year']\n",
    "    login_df['activity_duration'] = pd.cut(login_df['activity_duration'], bins=bins, labels=labels)\n",
    "    login_df = login_df[['jobma_catcher_id', 'activity_duration']]\n",
    "\n",
    "    return login_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "300d417e-cc6d-48e6-8034-abacd5ac162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_features(invitation_df, job_posting_df, kit_df):\n",
    "    print(\"Fetching features\")\n",
    "    for df in [invitation_df, job_posting_df, kit_df]:\n",
    "        if 'catcher_id' in df.columns:\n",
    "            df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    job_posting_df['job_posted'] = job_posting_df['jobma_catcher_id'].map(job_posting_df['jobma_catcher_id'].value_counts())\n",
    "    kit_df['number_of_kits'] = kit_df['jobma_catcher_id'].map(kit_df['jobma_catcher_id'].value_counts())\n",
    "\n",
    "    invitation_df = invitation_df[invitation_df['jobma_interview_status'] == '2']\n",
    "    invitation_df['number_of_invitations'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df.drop('jobma_interview_status', axis=1, inplace=True)\n",
    "    invitation_df['interview_completed'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df = invitation_df[invitation_df['jobma_interview_mode'].isin(['1', '2'])].copy()\n",
    "    interview_counts = invitation_df.groupby(['jobma_catcher_id', 'jobma_interview_mode']).size().unstack(fill_value=0)\n",
    "    interview_counts = interview_counts.rename(columns={'1': 'number_of_recorded_interviews', '2': 'number_of_live_interviews'})\n",
    "    invitation_df = invitation_df.merge(interview_counts, on='jobma_catcher_id', how='left')\n",
    "    \n",
    "    invitation_df.drop('jobma_interview_mode', axis=1, inplace=True)\n",
    "    invitation_df = invitation_df.drop_duplicates()\n",
    "\n",
    "    job_posting_df = job_posting_df[['jobma_catcher_id', 'job_posted']].drop_duplicates()\n",
    "    kit_df = kit_df[['jobma_catcher_id', 'number_of_kits']].drop_duplicates()\n",
    "\n",
    "    return invitation_df, job_posting_df, kit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec7196e-b17b-4c01-8893-1ad91b027830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df(catcher_df, wallet_df, subscription_df, invitation_df, job_posting_df, kit_df, login_df):\n",
    "    print(\"Merging DFs\")\n",
    "    final_df = catcher_df.copy()\n",
    "\n",
    "    # Left join each table one by one\n",
    "    final_df = final_df.merge(wallet_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(subscription_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(invitation_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(job_posting_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(kit_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(login_df, on='jobma_catcher_id', how='left')\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # For Total Sub\n",
    "    sub_counts = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent').size()\n",
    "    final_df['total_sub'] = final_df['jobma_catcher_id'].map(sub_counts).fillna(0).astype(int)\n",
    "\n",
    "    # For Kits\n",
    "    sub_kits_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_kits'].sum()\n",
    "    kits_mapped = final_df['jobma_catcher_id'].map(sub_kits_sum).fillna(0)\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].fillna(0) + kits_mapped\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].astype(int)\n",
    "    \n",
    "    # For Invitations\n",
    "    sub_invitations_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_invitations'].sum()\n",
    "    invitations_mapped = final_df['jobma_catcher_id'].map(sub_invitations_sum).fillna(0)\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].fillna(0) + invitations_mapped\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].astype(int)\n",
    "    \n",
    "    # For Job Posted\n",
    "    sub_job_posted_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['job_posted'].sum()\n",
    "    job_posted_mapped = final_df['jobma_catcher_id'].map(sub_job_posted_sum).fillna(0)\n",
    "    final_df['job_posted'] = final_df['job_posted'].fillna(0) + job_posted_mapped\n",
    "    final_df['job_posted'] = final_df['job_posted'].astype(int)\n",
    "\n",
    "    # For Recorded Interviews\n",
    "    sub_recorded_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_recorded_interviews'].sum()\n",
    "    recorded_mapped = final_df['jobma_catcher_id'].map(sub_recorded_sum).fillna(0)\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].fillna(0) + recorded_mapped\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].astype(int)\n",
    "\n",
    "    # For Live Interviews\n",
    "    sub_live_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_live_interviews'].sum()\n",
    "    live_mapped = final_df['jobma_catcher_id'].map(sub_live_sum).fillna(0)\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].fillna(0) + live_mapped\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].astype(int)\n",
    "\n",
    "    # For Interview Completed\n",
    "    sub_to_parent_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['interview_completed'].sum()\n",
    "    final_df.loc[final_df['jobma_catcher_id'].isin(sub_to_parent_sum.index), 'interview_completed'] += final_df['jobma_catcher_id'].map(sub_to_parent_sum).fillna(0).astype(int)\n",
    "\n",
    "    # For Minimum Login Days\n",
    "    login_order = {\n",
    "        'Less than 1 Week':0,\n",
    "        '1-4 Weeks':1,\n",
    "        '1-3 Months':2,\n",
    "        '3-6 Months':3,\n",
    "        '6-12 Months':4,\n",
    "        'More than 1 Year':5\n",
    "    }\n",
    "\n",
    "    final_df['activity_duration'] = final_df['activity_duration'].map(login_order).fillna(5).astype(int)\n",
    "    sub_min_login = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['activity_duration'].min()\n",
    "    final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'activity_duration'] = final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'jobma_catcher_id'].map(sub_min_login)\n",
    "\n",
    "    verified_df = final_df[final_df['jobma_verified'] == 1].copy()\n",
    "    df = verified_df[verified_df['jobma_catcher_parent'] == 0].copy()\n",
    "    df.drop(['jobma_catcher_parent', 'jobma_verified'], axis=1, inplace=True)\n",
    "    \n",
    "    compare_df = df.copy()\n",
    "    df.drop('jobma_catcher_id', axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Final merged df shape is {df.shape}\")\n",
    "\n",
    "    return df, compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d9439a-64d7-4ff9-8130-eb5d4a1e89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(final_df):\n",
    "    final_df = final_df.copy()\n",
    "\n",
    "    # Step 1: Replace inf with NaN first\n",
    "    final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Step 2: Fill NaNs\n",
    "    fill_values = {\n",
    "        'is_premium': 0,\n",
    "        'subscription_status': 0,\n",
    "        'company_size': '1-25',\n",
    "        'is_unlimited': 0,\n",
    "        'subscription_amount_in_dollars': 0,\n",
    "        'number_of_subscriptions': 0,\n",
    "        'number_of_invitations': 0,\n",
    "        'interview_completed': 0,\n",
    "        'number_of_recorded_interviews': 0,\n",
    "        'number_of_live_interviews': 0,\n",
    "        'job_posted': 0,\n",
    "        'number_of_kits': 0,\n",
    "        'activity_duration': 5,\n",
    "        'total_sub': 0,\n",
    "    }\n",
    "    final_df.fillna(fill_values, inplace=True)\n",
    "\n",
    "    # Step 3: Explicitly cast to int for the appropriate columns\n",
    "    int_columns = [\n",
    "        'is_premium',\n",
    "        'subscription_status',\n",
    "        'is_unlimited',\n",
    "        'number_of_subscriptions',\n",
    "        'jobma_interview_status',\n",
    "        'number_of_invitations',\n",
    "        'interview_completed',\n",
    "        'number_of_recorded_interviews',\n",
    "        'number_of_live_interviews',\n",
    "        'job_posted',\n",
    "        'number_of_kits',\n",
    "        'activity_duration',\n",
    "        'total_sub',\n",
    "    ]\n",
    "    for col in int_columns:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = pd.to_numeric(final_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65ca5eb-8cad-47c5-b691-b51cd28a0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Encoding \n",
    "\n",
    "def ordinal_encoder(df):\n",
    "    ordinal_col = ['company_size']\n",
    "    company_size_order = ['1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "\n",
    "    ordinal = OrdinalEncoder(categories=[company_size_order])\n",
    "\n",
    "    encoded = ordinal.fit_transform(df[ordinal_col].astype(str))\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f' {col}_ord' for col in ordinal_col], index=df.index)\n",
    "\n",
    "    df.drop(columns=ordinal_col, inplace=True)\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8687e49-865d-49a9-b5af-d4f9d41b12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation \n",
    "\n",
    "def log_transform(df):\n",
    "    log_cols = [\n",
    "        'subscription_amount_in_dollars',\n",
    "        'number_of_subscriptions',\n",
    "        'interview_completed',\n",
    "        'number_of_invitations',\n",
    "        'number_of_recorded_interviews',\n",
    "        'number_of_live_interviews',\n",
    "        'job_posted',\n",
    "        'number_of_kits',\n",
    "        'activity_duration'\n",
    "        'total_sub'\n",
    "    ]\n",
    "\n",
    "    df = df.copy()\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            # fill NaNs\n",
    "            df[col] = df[col].fillna(0)\n",
    "            # if a number is less than zero, turn it into zero\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "            # safe log1p\n",
    "            df[col] = np.log1p(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b5fc87-0e15-4f29-bb6e-eb3d6bc17d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30518571-d853-4dfb-b2be-322d761c0b4b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f5c6cc8-7334-4e7c-b1bf-1a54b6c3898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        engine = create_connection()\n",
    "        wallet_df,subscription_df,invitation_df,job_posting_df,kit_df, login_df = create_df(engine)\n",
    "        self.wallet_df = wallet_df\n",
    "        self.subscription_df = subscription_df\n",
    "        self.invitation_df = invitation_df\n",
    "        self.job_posting_df = job_posting_df\n",
    "        self.kit_df = kit_df\n",
    "        self.login_df = login_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, catcher_df):\n",
    "        catcher = fetching_catcher_df(catcher_df)\n",
    "        wallet = fetching_wallet_df(self.wallet_df)\n",
    "        subscription = fetching_subscription_df(self.subscription_df)\n",
    "        login = fetching_login_df(self.login_df)\n",
    "        invitation, job_posting, kit = fetching_features(\n",
    "            self.invitation_df,\n",
    "            self.job_posting_df,\n",
    "            self.kit_df,\n",
    "        )\n",
    "        final_df, compare_df = merging_df(catcher, wallet, subscription, invitation, job_posting, kit, login)\n",
    "        self.compare_df_ = fill_missing_values(compare_df)\n",
    "        self.compare_df_.to_csv(\"compare_df.csv\", index=False)\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc1d82b9-ae00-47b9-9115-9405d2422f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = fill_missing_values(X)\n",
    "        X = ordinal_encoder(X)\n",
    "        X = log_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "733a8914-50b9-4955-91f6-9706bc7a9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # bottleneck\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f7dab9-31f4-4983-b540-2e486b0f99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cc69fb8-7dd0-4973-be37-7802c5d1f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model():\n",
    "    print(\"Starting with model training\")\n",
    "    # Pipelines\n",
    "    merge_pipeline = Pipeline([\n",
    "        ('merge', MergingTransformer())\n",
    "    ])\n",
    "\n",
    "    # Step 2: Preprocess merged data\n",
    "    preprocess_pipeline = Pipeline([\n",
    "        ('preprocessing', PreprocessingTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('to_tensor', FunctionTransformer(to_tensor))\n",
    "    ])\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('merge_pipeline', merge_pipeline),\n",
    "        ('preprocess_pipeline', preprocess_pipeline)\n",
    "    ])\n",
    "    \n",
    "    #saving preprocessing pipeline\n",
    "    # dump(preprocess_pipeline, 'pipeline.joblib')\n",
    "\n",
    "    X_tensor = full_pipeline.fit_transform(create_df(create_connection(),catcher_only=True))\n",
    "    \n",
    "    with open('full_pipeline.pkl', 'wb') as f:\n",
    "        pickle.dump(full_pipeline, f)\n",
    "    \n",
    "    X_df = pd.DataFrame(X_tensor)\n",
    "    X_data = CustomDataset(X_tensor)\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    dataloader = DataLoader(X_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initializing the Model \n",
    "    input_shape = X_df.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "\n",
    "    # Important Parameters \n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    patience = 5\n",
    "    delta = 1e-4\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    training_losses = []\n",
    "\n",
    "    # Loss Function, Optimizers and LR Scheduler \n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.AdamW(model_1.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model_1.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            encoded, decoded = model_1(batch)\n",
    "            loss = loss_function(decoded, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        training_losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_loss < best_loss - delta:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Print current learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']:.6f}\")\n",
    "    print(\"Saving Model and Embeddings\")\n",
    "    torch.save(model_1.state_dict(),'model.pth')\n",
    "    encoder, _ = model_1(X_tensor)\n",
    "    latent_np = encoder.cpu().detach().numpy()\n",
    "\n",
    "    with open('latent_np.pkl', 'wb') as embeddings_file:\n",
    "        pickle.dump(latent_np, embeddings_file)\n",
    "\n",
    "    print(\"Model and Embeddings Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e355f837-7765-4823-81c0-e294020ad713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user_input, n=5):\n",
    "    with open('full_pipeline.pkl', 'rb') as f:\n",
    "        full_pipeline = pickle.load(f)\n",
    "\n",
    "    # Extract pipelines\n",
    "    merge_pipeline = full_pipeline.named_steps['merge_pipeline']\n",
    "    preprocess_pipeline = full_pipeline.named_steps['preprocess_pipeline']\n",
    "\n",
    "    # access compare_df using .csv file instead of fetching it from database\n",
    "    compare_df = pd.read_csv('compare_df.csv')\n",
    "\n",
    "    # Transform user input only with preprocessing pipeline\n",
    "    sample_input = pd.DataFrame([user_input])\n",
    "    transformed_input = preprocess_pipeline.transform(sample_input)\n",
    "\n",
    "    # Load trained model\n",
    "    input_shape = transformed_input.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "    model_1.load_state_dict(torch.load('model.pth'))\n",
    "    model_1.eval()\n",
    "\n",
    "    # Load latent embeddings\n",
    "    with open('latent_np.pkl', 'rb') as embeddings_file:\n",
    "        embeddings = pickle.load(embeddings_file)\n",
    "\n",
    "    # Generate user embedding\n",
    "    user_tensor = torch.tensor(transformed_input, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        user_embedding, _ = model_1(user_tensor)\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "\n",
    "        latent_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        latent_embeddings_tensor = F.normalize(latent_embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity and get top-N\n",
    "        similarities = F.cosine_similarity(user_embedding, latent_embeddings_tensor, dim=1)\n",
    "        top_indices = similarities.topk(n).indices.cpu().numpy()\n",
    "\n",
    "    # Final Recommendation DataFrame\n",
    "    recommended = compare_df.iloc[top_indices].copy()\n",
    "    recommended['similarity'] = similarities[top_indices].cpu().numpy()\n",
    "\n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1251f14a-6b1c-40d5-ae65-818e0354902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_good = {'is_premium':0,\n",
    "                 'subscription_status':0,\n",
    "                 'company_size':'101-500', \n",
    "                 'is_unlimited':0,\n",
    "                 'subscription_amount_in_dollars': 100.00,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':25,\n",
    "                'interview_completed':10,\n",
    "                'number_of_recorded_interviews':8,\n",
    "                'number_of_live_interviews':5,\n",
    "                'job_posted':4,\n",
    "                'number_of_kits':7,\n",
    "                'activity_duration':4,\n",
    "                'total_sub':1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b7ba3cf-d2f9-4700-8c74-a5bf0e60f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_test = {'is_premium':1,\n",
    "                 'subscription_status':1,\n",
    "                 'company_size':'1-25',\n",
    "                 'is_unlimited':1,\n",
    "                 'subscription_amount_in_dollars': 125.0,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':18,\n",
    "                  'interview_completed':10,\n",
    "                  'number_of_recorded_interviews':3,\n",
    "                'number_of_live_interviews':1,\n",
    "                 'job_posted':1,\n",
    "                 'number_of_kits':1,\n",
    "                'activity_duration':5,\n",
    "                'total_sub':1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d295c8f-7669-4b4a-b1ce-7b4b41a85e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Wallet DF\n",
      "Subscription DF\n",
      "Invitation DF\n",
      "Job posting DF\n",
      "kit DF\n",
      "Login DF\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Catcher DF is requested\n",
      "Processing catcher DF\n",
      "Processing wallet DF\n",
      "Processing subscription DF\n",
      "Processing login DF\n",
      "Fetching features\n",
      "Merging DFs\n",
      "Final merged df shape is (4463, 14)\n",
      "Epoch: 1/2 | Loss: 0.169690\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 2/2 | Loss: 0.083328\n",
      "Learning Rate: 0.001000\n",
      "Saving Model and Embeddings\n",
      "Model and Embeddings Saved\n"
     ]
    }
   ],
   "source": [
    "train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b42a753-8c27-464e-ba66-c31938cc734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobma_catcher_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>subscription_status</th>\n",
       "      <th>company_size</th>\n",
       "      <th>is_unlimited</th>\n",
       "      <th>subscription_amount_in_dollars</th>\n",
       "      <th>number_of_subscriptions</th>\n",
       "      <th>number_of_invitations</th>\n",
       "      <th>interview_completed</th>\n",
       "      <th>number_of_recorded_interviews</th>\n",
       "      <th>number_of_live_interviews</th>\n",
       "      <th>job_posted</th>\n",
       "      <th>number_of_kits</th>\n",
       "      <th>activity_duration</th>\n",
       "      <th>total_sub</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>7151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>346.122257</td>\n",
       "      <td>1</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>8219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3479</th>\n",
       "      <td>8056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>3635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>138.448903</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>8736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.973102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      jobma_catcher_id  is_premium  subscription_status company_size  \\\n",
       "2749              7151           0                    0       26-100   \n",
       "3578              8219           0                    0       26-100   \n",
       "3479              8056           0                    0       26-100   \n",
       "470               3635           0                    0       26-100   \n",
       "3757              8736           0                    0       26-100   \n",
       "\n",
       "      is_unlimited  subscription_amount_in_dollars  number_of_subscriptions  \\\n",
       "2749             0                      346.122257                        1   \n",
       "3578             0                     1500.000000                        1   \n",
       "3479             0                      150.000000                        1   \n",
       "470              0                      138.448903                        1   \n",
       "3757             0                      150.000000                        1   \n",
       "\n",
       "      number_of_invitations  interview_completed  \\\n",
       "2749                    347                  347   \n",
       "3578                      3                    3   \n",
       "3479                      2                    2   \n",
       "470                       3                    3   \n",
       "3757                     21                   21   \n",
       "\n",
       "      number_of_recorded_interviews  number_of_live_interviews  job_posted  \\\n",
       "2749                            347                          0           3   \n",
       "3578                              2                          0          10   \n",
       "3479                              2                          0           1   \n",
       "470                               3                          0           1   \n",
       "3757                             21                          0           7   \n",
       "\n",
       "      number_of_kits  activity_duration  total_sub  similarity  \n",
       "2749              22                  5          1    0.976815  \n",
       "3578               4                  5          1    0.976087  \n",
       "3479               1                  5          0    0.974534  \n",
       "470                1                  5          0    0.973108  \n",
       "3757               5                  5          4    0.973102  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(user_pref_good, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "186c09d0-38b2-4859-b97a-2069dd35541d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobma_catcher_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>subscription_status</th>\n",
       "      <th>company_size</th>\n",
       "      <th>is_unlimited</th>\n",
       "      <th>subscription_amount_in_dollars</th>\n",
       "      <th>number_of_subscriptions</th>\n",
       "      <th>number_of_invitations</th>\n",
       "      <th>interview_completed</th>\n",
       "      <th>number_of_recorded_interviews</th>\n",
       "      <th>number_of_live_interviews</th>\n",
       "      <th>job_posted</th>\n",
       "      <th>number_of_kits</th>\n",
       "      <th>activity_duration</th>\n",
       "      <th>total_sub</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>5868</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>2630.529156</td>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.939949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1-25</td>\n",
       "      <td>1</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.934791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>9994</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.932588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>10162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.929340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>9598</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>19205.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.926265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      jobma_catcher_id  is_premium  subscription_status company_size  \\\n",
       "1879              5868           1                    1         1-25   \n",
       "188               3206           1                    0         1-25   \n",
       "4120              9994           0                    1         1-25   \n",
       "4218             10162           1                    1         1-25   \n",
       "3874              9598           1                    1         1-25   \n",
       "\n",
       "      is_unlimited  subscription_amount_in_dollars  number_of_subscriptions  \\\n",
       "1879             0                     2630.529156                        8   \n",
       "188              1                      125.000000                        1   \n",
       "4120             1                        0.000000                        1   \n",
       "4218             0                       50.000000                        1   \n",
       "3874             0                    19205.000000                       11   \n",
       "\n",
       "      number_of_invitations  interview_completed  \\\n",
       "1879                    350                  350   \n",
       "188                       3                    3   \n",
       "4120                     74                   74   \n",
       "4218                      9                    9   \n",
       "3874                     63                   63   \n",
       "\n",
       "      number_of_recorded_interviews  number_of_live_interviews  job_posted  \\\n",
       "1879                            350                          0          13   \n",
       "188                               2                          1           3   \n",
       "4120                             48                         26          24   \n",
       "4218                              9                          0           2   \n",
       "3874                             63                          0          21   \n",
       "\n",
       "      number_of_kits  activity_duration  total_sub  similarity  \n",
       "1879              44                  5          4    0.939949  \n",
       "188                3                  5          2    0.934791  \n",
       "4120               5                  5          2    0.932588  \n",
       "4218               8                  5          2    0.929340  \n",
       "3874              12                  5          2    0.926265  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(user_pref_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef1390-900a-4a1e-adcd-198c55749d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
