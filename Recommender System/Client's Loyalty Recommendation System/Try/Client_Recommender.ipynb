{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f7edda-c240-4f29-9c57-fe5e9b144bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data Encoding and Scaling\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92ccb96-f858-4600-9f68-b0f9f784e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db8d71d-3686-4446-b3df-69a80d0aa868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading .env file into my python code\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccaf725-a5f6-4790-82ac-d3fa685c0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    print('creating connection with DB')\n",
    "    user = os.getenv(\"DB_USER\")\n",
    "    raw_password = os.getenv(\"DB_PASSWORD\")\n",
    "    password = quote_plus(raw_password)\n",
    "    host = os.getenv(\"DB_HOST\")\n",
    "    port = os.getenv(\"DB_PORT\")\n",
    "    db = os.getenv(\"DB_NAME\")\n",
    "\n",
    "    # Credentials of mySQL connection\n",
    "    connection_string = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print('connection created successfully')\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697791f2-b367-430e-9357-1dc380ffc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(engine,catcher_only=False):\n",
    "    print('creating DFs=============')\n",
    "    if catcher_only:\n",
    "        print('Catcher DF is requested')\n",
    "        catcher_df = pd.read_sql('Select jobma_catcher_id, is_premium, jobma_catcher_parent, jobma_verified, subscription_status, company_size FROM jobma_catcher', con=engine) \n",
    "        return catcher_df\n",
    "    print(\"Wallet DF\")\n",
    "    wallet_df = pd.read_sql('Select catcher_id, is_unlimited FROM wallet', con=engine)\n",
    "    print(\"Subscription DF\")\n",
    "    subscription_df = pd.read_sql('Select catcher_id, currency, subscription_amount FROM subscription_history', con=engine)\n",
    "    print(\"Invitation DF\")\n",
    "    invitation_df = pd.read_sql('Select jobma_catcher_id, jobma_interview_mode, jobma_interview_status FROM jobma_pitcher_invitations', con=engine)\n",
    "    print(\"Job posting DF\")\n",
    "    job_posting_df = pd.read_sql('Select jobma_catcher_id FROM jobma_employer_job_posting', con=engine)\n",
    "    print(\"kit DF\")\n",
    "    kit_df = pd.read_sql('Select catcher_id FROM job_assessment_kit', con=engine)\n",
    "    print('Login DF')\n",
    "    login_df = pd.read_sql('Select jobma_role_id, jobma_user_id, jobma_last_login FROM jobma_login',con=engine)\n",
    "    # Closing the Connection\n",
    "    engine.dispose()\n",
    "    return wallet_df,subscription_df,invitation_df,job_posting_df,kit_df,login_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffd9ae-a8b3-4496-9070-eb5e64fa6d7a",
   "metadata": {},
   "source": [
    "# Specific Methods\n",
    "\n",
    "**To Fetch Columns from different tables and fitting those functions into Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09161e27-46da-4c2e-99f1-a48a74aa8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcher_df\n",
    "\n",
    "def fetching_catcher_df(catcher_df):\n",
    "    print(\"Processing catcher DF\")\n",
    "    catcher_df['jobma_verified'] = catcher_df['jobma_verified'].replace({'0':0, '1':1})\n",
    "    catcher_df.drop(catcher_df[catcher_df['is_premium'] == ''].index, inplace=True)\n",
    "    catcher_df['is_premium'] = catcher_df['is_premium'].replace({'0':0, '1':1})\n",
    "    catcher_df['subscription_status'] = catcher_df['subscription_status'].replace({'0':0, '1':1, '2':0})\n",
    "\n",
    "    return catcher_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658cf34a-8580-4fd0-854a-ad4ec8aa5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_df\n",
    "\n",
    "def fetching_wallet_df(wallet_df):\n",
    "    print(\"Processing wallet DF\")\n",
    "    wallet_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    wallet_df['is_unlimited'] = wallet_df['is_unlimited'].replace({'0':0, '1':1})\n",
    "    wallet_df = wallet_df[wallet_df['is_unlimited'] != '']\n",
    "    wallet_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return wallet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ae4bef-6e45-46b9-9a9d-5dd5aad3df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_df\n",
    "\n",
    "def fetching_subscription_df(subscription_df):\n",
    "    print(\"Processing subscription DF\")\n",
    "    subscription_df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "    subscription_df.loc[subscription_df['currency'] == '1', 'subscription_amount'] /= 85.23\n",
    "    subscription_df = subscription_df.groupby('jobma_catcher_id').agg(\n",
    "        subscription_amount_in_dollars = ('subscription_amount', 'sum'),\n",
    "        number_of_subscriptions = ('subscription_amount', 'count'),\n",
    "    ).reset_index()\n",
    "    subscription_df.drop_duplicates(inplace=True)\n",
    "    return subscription_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611c17cc-9930-4585-9bb6-62fcbfecf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login_df\n",
    "\n",
    "def fetching_login_df(login_df):\n",
    "    print(\"Processing login DF\")\n",
    "    login_df = login_df[login_df['jobma_role_id'] == 3].copy()\n",
    "    login_df.rename(columns={'jobma_user_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    # Calculating Number of Gaps between last login and today\n",
    "    login_df['jobma_last_login'] = pd.to_datetime(login_df['jobma_last_login'], errors='coerce')\n",
    "    login_df['activity_duration'] = (pd.Timestamp('today') - login_df['jobma_last_login']).dt.days\n",
    "    login_df['activity_duration'].fillna(5, inplace=True)\n",
    "    login_df['activity_duration'] = login_df['activity_duration'].astype(int)\n",
    "\n",
    "    # Binning\n",
    "    bins = [-1,7,30,90,180,365,float('inf')]\n",
    "    labels = ['Less than 1 Week', '1-4 Weeks', '1-3 Months', '3-6 Months', '6-12 Months', 'More than 1 Year']\n",
    "    login_df['activity_duration'] = pd.cut(login_df['activity_duration'], bins=bins, labels=labels)\n",
    "    login_df = login_df[['jobma_catcher_id', 'activity_duration']]\n",
    "\n",
    "    return login_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "300d417e-cc6d-48e6-8034-abacd5ac162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_features(invitation_df, job_posting_df, kit_df):\n",
    "    print(\"Fetching features\")\n",
    "    for df in [invitation_df, job_posting_df, kit_df]:\n",
    "        if 'catcher_id' in df.columns:\n",
    "            df.rename(columns={'catcher_id': 'jobma_catcher_id'}, inplace=True)\n",
    "\n",
    "    job_posting_df['job_posted'] = job_posting_df['jobma_catcher_id'].map(job_posting_df['jobma_catcher_id'].value_counts())\n",
    "    kit_df['number_of_kits'] = kit_df['jobma_catcher_id'].map(kit_df['jobma_catcher_id'].value_counts())\n",
    "\n",
    "    invitation_df = invitation_df[invitation_df['jobma_interview_status'] == '2']\n",
    "    invitation_df['number_of_invitations'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df.drop('jobma_interview_status', axis=1, inplace=True)\n",
    "    invitation_df['interview_completed'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "    invitation_df = invitation_df[invitation_df['jobma_interview_mode'].isin(['1', '2'])].copy()\n",
    "    interview_counts = invitation_df.groupby(['jobma_catcher_id', 'jobma_interview_mode']).size().unstack(fill_value=0)\n",
    "    interview_counts = interview_counts.rename(columns={'1': 'number_of_recorded_interviews', '2': 'number_of_live_interviews'})\n",
    "    invitation_df = invitation_df.merge(interview_counts, on='jobma_catcher_id', how='left')\n",
    "    \n",
    "    invitation_df.drop('jobma_interview_mode', axis=1, inplace=True)\n",
    "    invitation_df = invitation_df.drop_duplicates()\n",
    "\n",
    "    job_posting_df = job_posting_df[['jobma_catcher_id', 'job_posted']].drop_duplicates()\n",
    "    kit_df = kit_df[['jobma_catcher_id', 'number_of_kits']].drop_duplicates()\n",
    "\n",
    "    return invitation_df, job_posting_df, kit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec7196e-b17b-4c01-8893-1ad91b027830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df(catcher_df, wallet_df, subscription_df, invitation_df, job_posting_df, kit_df, login_df):\n",
    "    print(\"Merging DFs\")\n",
    "    final_df = catcher_df.copy()\n",
    "\n",
    "    # Left join each table one by one\n",
    "    final_df = final_df.merge(wallet_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(subscription_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(invitation_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(job_posting_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(kit_df, on='jobma_catcher_id', how='left')\n",
    "    final_df = final_df.merge(login_df, on='jobma_catcher_id', how='left')\n",
    "    final_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # For Total Sub\n",
    "    sub_counts = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent').size()\n",
    "    final_df['total_sub'] = final_df['jobma_catcher_id'].map(sub_counts).fillna(0).astype(int)\n",
    "\n",
    "    # For Kits\n",
    "    sub_kits_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_kits'].sum()\n",
    "    kits_mapped = final_df['jobma_catcher_id'].map(sub_kits_sum).fillna(0)\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].fillna(0) + kits_mapped\n",
    "    final_df['number_of_kits'] = final_df['number_of_kits'].astype(int)\n",
    "    \n",
    "    # For Invitations\n",
    "    sub_invitations_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_invitations'].sum()\n",
    "    invitations_mapped = final_df['jobma_catcher_id'].map(sub_invitations_sum).fillna(0)\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].fillna(0) + invitations_mapped\n",
    "    final_df['number_of_invitations'] = final_df['number_of_invitations'].astype(int)\n",
    "    \n",
    "    # For Job Posted\n",
    "    sub_job_posted_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['job_posted'].sum()\n",
    "    job_posted_mapped = final_df['jobma_catcher_id'].map(sub_job_posted_sum).fillna(0)\n",
    "    final_df['job_posted'] = final_df['job_posted'].fillna(0) + job_posted_mapped\n",
    "    final_df['job_posted'] = final_df['job_posted'].astype(int)\n",
    "\n",
    "    # For Recorded Interviews\n",
    "    sub_recorded_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_recorded_interviews'].sum()\n",
    "    recorded_mapped = final_df['jobma_catcher_id'].map(sub_recorded_sum).fillna(0)\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].fillna(0) + recorded_mapped\n",
    "    final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].astype(int)\n",
    "\n",
    "    # For Live Interviews\n",
    "    sub_live_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_live_interviews'].sum()\n",
    "    live_mapped = final_df['jobma_catcher_id'].map(sub_live_sum).fillna(0)\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].fillna(0) + live_mapped\n",
    "    final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].astype(int)\n",
    "\n",
    "    # For Interview Completed\n",
    "    sub_to_parent_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['interview_completed'].sum()\n",
    "    final_df.loc[final_df['jobma_catcher_id'].isin(sub_to_parent_sum.index), 'interview_completed'] += final_df['jobma_catcher_id'].map(sub_to_parent_sum).fillna(0).astype(int)\n",
    "\n",
    "    # For Minimum Login Days\n",
    "    login_order = {\n",
    "        'Less than 1 Week':0,\n",
    "        '1-4 Weeks':1,\n",
    "        '1-3 Months':2,\n",
    "        '3-6 Months':3,\n",
    "        '6-12 Months':4,\n",
    "        'More than 1 Year':5\n",
    "    }\n",
    "\n",
    "    final_df['activity_duration'] = final_df['activity_duration'].map(login_order).fillna(5).astype(int)\n",
    "    sub_min_login = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['activity_duration'].min()\n",
    "    final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'activity_duration'] = final_df.loc[final_df['jobma_catcher_id'].isin(sub_min_login.index), 'jobma_catcher_id'].map(sub_min_login)\n",
    "\n",
    "    verified_df = final_df[final_df['jobma_verified'] == 1].copy()\n",
    "    df = verified_df[verified_df['jobma_catcher_parent'] == 0].copy()\n",
    "    df.drop(['jobma_catcher_parent', 'jobma_verified'], axis=1, inplace=True)\n",
    "    \n",
    "    compare_df = df.copy()\n",
    "    df.drop('jobma_catcher_id', axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Final merged df shape is {df.shape}\")\n",
    "\n",
    "    return df, compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d9439a-64d7-4ff9-8130-eb5d4a1e89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(final_df):\n",
    "    final_df = final_df.copy()\n",
    "\n",
    "    # Step 1: Replace inf with NaN first\n",
    "    final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Step 2: Fill NaNs\n",
    "    fill_values = {\n",
    "        'is_premium': 0,\n",
    "        'subscription_status': 0,\n",
    "        'company_size': '1-25',\n",
    "        'is_unlimited': 0,\n",
    "        'subscription_amount_in_dollars': 0,\n",
    "        'number_of_subscriptions': 0,\n",
    "        'number_of_invitations': 0,\n",
    "        'interview_completed': 0,\n",
    "        'number_of_recorded_interviews': 0,\n",
    "        'number_of_live_interviews': 0,\n",
    "        'job_posted': 0,\n",
    "        'number_of_kits': 0,\n",
    "        'activity_duration': 5,\n",
    "        'total_sub': 0,\n",
    "    }\n",
    "    final_df.fillna(fill_values, inplace=True)\n",
    "\n",
    "    # Step 3: Explicitly cast to int for the appropriate columns\n",
    "    int_columns = [\n",
    "        'is_premium',\n",
    "        'subscription_status',\n",
    "        'is_unlimited',\n",
    "        'number_of_subscriptions',\n",
    "        'jobma_interview_status',\n",
    "        'number_of_invitations',\n",
    "        'interview_completed',\n",
    "        'number_of_recorded_interviews',\n",
    "        'number_of_live_interviews',\n",
    "        'job_posted',\n",
    "        'number_of_kits',\n",
    "        'activity_duration',\n",
    "        'total_sub',\n",
    "    ]\n",
    "    for col in int_columns:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = pd.to_numeric(final_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65ca5eb-8cad-47c5-b691-b51cd28a0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Encoding \n",
    "\n",
    "def ordinal_encoder(df):\n",
    "    ordinal_col = ['company_size']\n",
    "    company_size_order = ['1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "\n",
    "    ordinal = OrdinalEncoder(categories=[company_size_order])\n",
    "\n",
    "    encoded = ordinal.fit_transform(df[ordinal_col].astype(str))\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f' {col}_ord' for col in ordinal_col], index=df.index)\n",
    "\n",
    "    df.drop(columns=ordinal_col, inplace=True)\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8687e49-865d-49a9-b5af-d4f9d41b12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation \n",
    "\n",
    "def log_transform(df):\n",
    "    log_cols = [\n",
    "        'subscription_amount_in_dollars',\n",
    "        'number_of_subscriptions',\n",
    "        'interview_completed',\n",
    "        'number_of_invitations',\n",
    "        'number_of_recorded_interviews',\n",
    "        'number_of_live_interviews',\n",
    "        'job_posted',\n",
    "        'number_of_kits',\n",
    "        'activity_duration'\n",
    "        'total_sub'\n",
    "    ]\n",
    "\n",
    "    df = df.copy()\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            # fill NaNs\n",
    "            df[col] = df[col].fillna(0)\n",
    "            # if a number is less than zero, turn it into zero\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "            # safe log1p\n",
    "            df[col] = np.log1p(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b5fc87-0e15-4f29-bb6e-eb3d6bc17d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30518571-d853-4dfb-b2be-322d761c0b4b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f5c6cc8-7334-4e7c-b1bf-1a54b6c3898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        engine = create_connection()\n",
    "        wallet_df,subscription_df,invitation_df,job_posting_df,kit_df, login_df = create_df(engine)\n",
    "        self.wallet_df = wallet_df\n",
    "        self.subscription_df = subscription_df\n",
    "        self.invitation_df = invitation_df\n",
    "        self.job_posting_df = job_posting_df\n",
    "        self.kit_df = kit_df\n",
    "        self.login_df = login_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, catcher_df):\n",
    "        catcher = fetching_catcher_df(catcher_df)\n",
    "        wallet = fetching_wallet_df(self.wallet_df)\n",
    "        subscription = fetching_subscription_df(self.subscription_df)\n",
    "        login = fetching_login_df(self.login_df)\n",
    "        invitation, job_posting, kit = fetching_features(\n",
    "            self.invitation_df,\n",
    "            self.job_posting_df,\n",
    "            self.kit_df,\n",
    "        )\n",
    "        final_df, compare_df = merging_df(catcher, wallet, subscription, invitation, job_posting, kit, login)\n",
    "        self.compare_df_ = fill_missing_values(compare_df)\n",
    "        self.compare_df_.to_csv(\"compare_df.csv\", index=False)\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc1d82b9-ae00-47b9-9115-9405d2422f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = fill_missing_values(X)\n",
    "        X = ordinal_encoder(X)\n",
    "        X = log_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "733a8914-50b9-4955-91f6-9706bc7a9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # bottleneck\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f7dab9-31f4-4983-b540-2e486b0f99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cc69fb8-7dd0-4973-be37-7802c5d1f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model():\n",
    "    print(\"Starting with model training\")\n",
    "    # Pipelines\n",
    "    merge_pipeline = Pipeline([\n",
    "        ('merge', MergingTransformer())\n",
    "    ])\n",
    "\n",
    "    # Step 2: Preprocess merged data\n",
    "    preprocess_pipeline = Pipeline([\n",
    "        ('preprocessing', PreprocessingTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('to_tensor', FunctionTransformer(to_tensor))\n",
    "    ])\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('merge_pipeline', merge_pipeline),\n",
    "        ('preprocess_pipeline', preprocess_pipeline)\n",
    "    ])\n",
    "    \n",
    "    #saving preprocessing pipeline\n",
    "    # dump(preprocess_pipeline, 'pipeline.joblib')\n",
    "\n",
    "    X_tensor = full_pipeline.fit_transform(create_df(create_connection(),catcher_only=True))\n",
    "    \n",
    "    with open('full_pipeline.pkl', 'wb') as f:\n",
    "        pickle.dump(full_pipeline, f)\n",
    "    \n",
    "    X_df = pd.DataFrame(X_tensor)\n",
    "    X_data = CustomDataset(X_tensor)\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    dataloader = DataLoader(X_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initializing the Model \n",
    "    input_shape = X_df.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "\n",
    "    # Important Parameters \n",
    "    learning_rate = 0.001\n",
    "    epochs = 50\n",
    "    patience = 5\n",
    "    delta = 1e-4\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    training_losses = []\n",
    "\n",
    "    # Loss Function, Optimizers and LR Scheduler \n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.AdamW(model_1.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model_1.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            encoded, decoded = model_1(batch)\n",
    "            loss = loss_function(decoded, batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        training_losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_loss < best_loss - delta:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Print current learning rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']:.6f}\")\n",
    "    print(\"Saving Model and Embeddings\")\n",
    "    torch.save(model_1.state_dict(),'model.pth')\n",
    "    encoder, _ = model_1(X_tensor)\n",
    "    latent_np = encoder.cpu().detach().numpy()\n",
    "\n",
    "    with open('latent_np.pkl', 'wb') as embeddings_file:\n",
    "        pickle.dump(latent_np, embeddings_file)\n",
    "\n",
    "    print(\"Model and Embeddings Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e355f837-7765-4823-81c0-e294020ad713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user_input, n=5):\n",
    "    with open('full_pipeline.pkl', 'rb') as f:\n",
    "        full_pipeline = pickle.load(f)\n",
    "\n",
    "    # Extract pipelines\n",
    "    merge_pipeline = full_pipeline.named_steps['merge_pipeline']\n",
    "    preprocess_pipeline = full_pipeline.named_steps['preprocess_pipeline']\n",
    "\n",
    "    # access compare_df using .csv file instead of fetching it from database\n",
    "    compare_df = pd.read_csv('compare_df.csv')\n",
    "\n",
    "    # Transform user input only with preprocessing pipeline\n",
    "    sample_input = pd.DataFrame([user_input])\n",
    "    transformed_input = preprocess_pipeline.transform(sample_input)\n",
    "\n",
    "    # Load trained model\n",
    "    input_shape = transformed_input.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "    model_1.load_state_dict(torch.load('model.pth'))\n",
    "    model_1.eval()\n",
    "\n",
    "    # Load latent embeddings\n",
    "    with open('latent_np.pkl', 'rb') as embeddings_file:\n",
    "        embeddings = pickle.load(embeddings_file)\n",
    "\n",
    "    # Generate user embedding\n",
    "    user_tensor = torch.tensor(transformed_input, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        user_embedding, _ = model_1(user_tensor)\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "\n",
    "        latent_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        latent_embeddings_tensor = F.normalize(latent_embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity and get top-N\n",
    "        similarities = F.cosine_similarity(user_embedding, latent_embeddings_tensor, dim=1)\n",
    "        top_indices = similarities.topk(n).indices.cpu().numpy()\n",
    "\n",
    "    # Final Recommendation DataFrame\n",
    "    recommended = compare_df.iloc[top_indices].copy()\n",
    "    recommended['similarity'] = similarities[top_indices].cpu().numpy()\n",
    "\n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1251f14a-6b1c-40d5-ae65-818e0354902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_good = {'is_premium':0,\n",
    "                 'subscription_status':0,\n",
    "                 'company_size':'101-500', \n",
    "                 'is_unlimited':0,\n",
    "                 'subscription_amount_in_dollars': 100.00,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':25,\n",
    "                'interview_completed':10,\n",
    "                'number_of_recorded_interviews':8,\n",
    "                'number_of_live_interviews':5,\n",
    "                'job_posted':4,\n",
    "                'number_of_kits':7,\n",
    "                'activity_duration':4,\n",
    "                'total_sub':1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b7ba3cf-d2f9-4700-8c74-a5bf0e60f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_test = {'is_premium':1,\n",
    "                  'subscription_status':1,\n",
    "                'company_size':'1-25',\n",
    "                 'is_unlimited':1,\n",
    "                 'subscription_amount_in_dollars': 125.0,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':18,\n",
    "                 'interview_completed':10,\n",
    "                  'number_of_recorded_interviews':523,\n",
    "                'number_of_live_interviews':1,\n",
    "                 'job_posted':1,\n",
    "                 'number_of_kits':1,\n",
    "                'activity_duration':5,\n",
    "                'total_sub':1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d295c8f-7669-4b4a-b1ce-7b4b41a85e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Wallet DF\n",
      "Subscription DF\n",
      "Invitation DF\n",
      "Job posting DF\n",
      "kit DF\n",
      "Login DF\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Catcher DF is requested\n",
      "Processing catcher DF\n",
      "Processing wallet DF\n",
      "Processing subscription DF\n",
      "Processing login DF\n",
      "Fetching features\n",
      "Merging DFs\n",
      "Final merged df shape is (4463, 14)\n",
      "Epoch: 1/50 | Loss: 0.166133\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 2/50 | Loss: 0.079728\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 3/50 | Loss: 0.059663\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 4/50 | Loss: 0.051799\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 5/50 | Loss: 0.047745\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 6/50 | Loss: 0.043838\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 7/50 | Loss: 0.042295\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 8/50 | Loss: 0.038744\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 9/50 | Loss: 0.037883\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 10/50 | Loss: 0.037643\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 11/50 | Loss: 0.034793\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 12/50 | Loss: 0.035006\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 13/50 | Loss: 0.034681\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 14/50 | Loss: 0.032985\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 15/50 | Loss: 0.031209\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 16/50 | Loss: 0.031000\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 17/50 | Loss: 0.029611\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 18/50 | Loss: 0.029517\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 19/50 | Loss: 0.029458\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 20/50 | Loss: 0.028536\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 21/50 | Loss: 0.026631\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 22/50 | Loss: 0.027274\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 23/50 | Loss: 0.026159\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 24/50 | Loss: 0.026228\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 25/50 | Loss: 0.025165\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 26/50 | Loss: 0.025178\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 27/50 | Loss: 0.025127\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 28/50 | Loss: 0.025219\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 29/50 | Loss: 0.025050\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 30/50 | Loss: 0.024800\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 31/50 | Loss: 0.024625\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 32/50 | Loss: 0.024791\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 33/50 | Loss: 0.023920\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 34/50 | Loss: 0.024242\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 35/50 | Loss: 0.023766\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 36/50 | Loss: 0.023471\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 37/50 | Loss: 0.021450\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 38/50 | Loss: 0.022598\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 39/50 | Loss: 0.020563\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 40/50 | Loss: 0.020963\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 41/50 | Loss: 0.021418\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 42/50 | Loss: 0.020406\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 43/50 | Loss: 0.020765\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 44/50 | Loss: 0.021296\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 45/50 | Loss: 0.019736\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 46/50 | Loss: 0.019370\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 47/50 | Loss: 0.019646\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 48/50 | Loss: 0.019358\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 49/50 | Loss: 0.017815\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 50/50 | Loss: 0.019352\n",
      "Learning Rate: 0.001000\n",
      "Saving Model and Embeddings\n",
      "Model and Embeddings Saved\n"
     ]
    }
   ],
   "source": [
    "train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b42a753-8c27-464e-ba66-c31938cc734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobma_catcher_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>subscription_status</th>\n",
       "      <th>company_size</th>\n",
       "      <th>is_unlimited</th>\n",
       "      <th>subscription_amount_in_dollars</th>\n",
       "      <th>number_of_subscriptions</th>\n",
       "      <th>number_of_invitations</th>\n",
       "      <th>interview_completed</th>\n",
       "      <th>number_of_recorded_interviews</th>\n",
       "      <th>number_of_live_interviews</th>\n",
       "      <th>job_posted</th>\n",
       "      <th>number_of_kits</th>\n",
       "      <th>activity_duration</th>\n",
       "      <th>total_sub</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>10499</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>250.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.949224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>3394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>212.84489</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>10482</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101-500</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>10479</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>10442</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26-100</td>\n",
       "      <td>0</td>\n",
       "      <td>150.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      jobma_catcher_id  is_premium  subscription_status company_size  \\\n",
       "4434             10499           0                    1         1-25   \n",
       "315               3394           0                    0       26-100   \n",
       "4426             10482           0                    1      101-500   \n",
       "4424             10479           1                    1         1-25   \n",
       "4405             10442           0                    1       26-100   \n",
       "\n",
       "      is_unlimited  subscription_amount_in_dollars  number_of_subscriptions  \\\n",
       "4434             0                       250.00000                        2   \n",
       "315              0                       212.84489                        2   \n",
       "4426             0                       100.00000                        1   \n",
       "4424             0                         0.00000                        1   \n",
       "4405             0                       150.00000                        1   \n",
       "\n",
       "      number_of_invitations  interview_completed  \\\n",
       "4434                     12                   12   \n",
       "315                       9                    9   \n",
       "4426                     14                   14   \n",
       "4424                      3                    3   \n",
       "4405                     69                   69   \n",
       "\n",
       "      number_of_recorded_interviews  number_of_live_interviews  job_posted  \\\n",
       "4434                              9                          3           2   \n",
       "315                               1                          8           4   \n",
       "4426                              9                          5           4   \n",
       "4424                              2                          1           3   \n",
       "4405                             67                          2          16   \n",
       "\n",
       "      number_of_kits  activity_duration  total_sub  similarity  \n",
       "4434               2                  5          1    0.949224  \n",
       "315                3                  5          1    0.930801  \n",
       "4426               7                  5          1    0.892023  \n",
       "4424               3                  5          2    0.855778  \n",
       "4405              31                  5          2    0.846653  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(user_pref_good, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "186c09d0-38b2-4859-b97a-2069dd35541d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobma_catcher_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>subscription_status</th>\n",
       "      <th>company_size</th>\n",
       "      <th>is_unlimited</th>\n",
       "      <th>subscription_amount_in_dollars</th>\n",
       "      <th>number_of_subscriptions</th>\n",
       "      <th>number_of_invitations</th>\n",
       "      <th>interview_completed</th>\n",
       "      <th>number_of_recorded_interviews</th>\n",
       "      <th>number_of_live_interviews</th>\n",
       "      <th>job_posted</th>\n",
       "      <th>number_of_kits</th>\n",
       "      <th>activity_duration</th>\n",
       "      <th>total_sub</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>10162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.788208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>10028</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.782981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>5526</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>1</td>\n",
       "      <td>10234.659158</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.762875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>5417</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26-100</td>\n",
       "      <td>1</td>\n",
       "      <td>20150.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.756415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>6475</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-25</td>\n",
       "      <td>0</td>\n",
       "      <td>1038.366772</td>\n",
       "      <td>3</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>638</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.749106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      jobma_catcher_id  is_premium  subscription_status company_size  \\\n",
       "4218             10162           1                    1         1-25   \n",
       "4134             10028           1                    1         1-25   \n",
       "1659              5526           1                    0       26-100   \n",
       "1585              5417           1                    0       26-100   \n",
       "2273              6475           1                    1         1-25   \n",
       "\n",
       "      is_unlimited  subscription_amount_in_dollars  number_of_subscriptions  \\\n",
       "4218             0                       50.000000                        1   \n",
       "4134             0                        0.000000                        2   \n",
       "1659             1                    10234.659158                        3   \n",
       "1585             1                    20150.000000                        6   \n",
       "2273             0                     1038.366772                        3   \n",
       "\n",
       "      number_of_invitations  interview_completed  \\\n",
       "4218                      9                    9   \n",
       "4134                     14                   14   \n",
       "1659                     34                   34   \n",
       "1585                     47                   47   \n",
       "2273                    638                  638   \n",
       "\n",
       "      number_of_recorded_interviews  number_of_live_interviews  job_posted  \\\n",
       "4218                              9                          0           2   \n",
       "4134                             14                          0           1   \n",
       "1659                             34                          0           4   \n",
       "1585                             47                          0           5   \n",
       "2273                            638                          0          69   \n",
       "\n",
       "      number_of_kits  activity_duration  total_sub  similarity  \n",
       "4218               8                  5          2    0.788208  \n",
       "4134               4                  5          2    0.782981  \n",
       "1659               4                  5          3    0.762875  \n",
       "1585               6                  5          1    0.756415  \n",
       "2273              91                  5          5    0.749106  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(user_pref_test, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
