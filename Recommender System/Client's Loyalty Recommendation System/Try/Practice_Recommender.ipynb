{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c51e8dc-c33a-43c9-91a2-1f65fb92965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half Pyramid of Terror\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn.functional as F\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d550f087-874a-4655-8c32-ca5624a62c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67f28be-4ef9-44d1-ba26-7acffbd5fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading .env file into my python code\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27c48a2-eb27-422d-9ab9-b71b280d6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    print('creating connection with DB')\n",
    "    try:\n",
    "        user = os.getenv(\"DB_USER\")\n",
    "        raw_password = os.getenv(\"DB_PASSWORD\")\n",
    "        password = quote_plus(raw_password)\n",
    "        host = os.getenv(\"DB_HOST\")\n",
    "        port = os.getenv(\"DB_PORT\")\n",
    "        db = os.getenv(\"DB_NAME\")\n",
    "    \n",
    "        # Credentials of mySQL connection\n",
    "        connection_string = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        print('connection created successfully')\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f'Error creating connection with DB: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c54cad2-da3b-4a26-a3f1-be6538558e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(engine,catcher_only=False):\n",
    "    print('creating DFs=============')\n",
    "    try:\n",
    "        if catcher_only:\n",
    "            print('Catcher DF is requested')\n",
    "            catcher_df = pd.read_sql(\"Select jobma_catcher_id, jobma_catcher_parent, subscription_status, company_size FROM jobma_catcher where jobma_verified = '1' \", con=engine) \n",
    "            return catcher_df\n",
    "        print(\"Wallet DF\")\n",
    "        wallet_df = pd.read_sql(\"Select catcher_id AS jobma_catcher_id, is_unlimited FROM wallet where is_unlimited <> '' \", con=engine)\n",
    "        print(\"Subscription DF\")\n",
    "        subscription_df = pd.read_sql(\"Select catcher_id AS jobma_catcher_id, currency, subscription_amount FROM subscription_history\", con=engine)\n",
    "        print(\"Invitation DF\")\n",
    "        invitation_df = pd.read_sql(\"Select jobma_catcher_id, jobma_interview_mode, jobma_interview_status FROM jobma_pitcher_invitations\", con=engine)\n",
    "        print(\"Job posting DF\")\n",
    "        job_posting_df = pd.read_sql(\"Select jobma_catcher_id FROM jobma_employer_job_posting\", con=engine)\n",
    "        print(\"kit DF\")\n",
    "        kit_df = pd.read_sql(\"Select catcher_id AS jobma_catcher_id FROM job_assessment_kit\", con=engine)\n",
    "        print('Login DF')\n",
    "        login_df = pd.read_sql(\"Select jobma_user_id AS jobma_catcher_id, jobma_last_login FROM jobma_login where jobma_role_id = 3\",con=engine)\n",
    "        \n",
    "        # Closing the Connection\n",
    "        engine.dispose()\n",
    "        return wallet_df, subscription_df,invitation_df,job_posting_df,kit_df,login_df\n",
    "    except Exception as e:\n",
    "        print(f'Error in create_df: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b13d081-936f-4124-8ce4-aef3122ab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catcher_df\n",
    "\n",
    "def fetching_catcher_df(catcher_df):\n",
    "    print(\"Processing catcher DF\")\n",
    "    try:\n",
    "        catcher_df['subscription_status'] = catcher_df['subscription_status'].replace({'0':0, '1':1, '2':0})\n",
    "        return catcher_df\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bac1fc3-a740-473a-9b6b-d6b04a9366c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_df\n",
    "\n",
    "def fetching_wallet_df(wallet_df):\n",
    "    print(\"Processing wallet DF\")\n",
    "    try:\n",
    "        wallet_df['is_unlimited'] = wallet_df['is_unlimited'].replace({'0':0, '1':1})\n",
    "        wallet_df.drop_duplicates(inplace=True)\n",
    "        return wallet_df\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316bcb04-f490-46ae-ad59-bdf5305d6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscription_df\n",
    "\n",
    "def fetching_subscription_df(subscription_df):\n",
    "    print(\"Processing subscription DF\")\n",
    "    try:\n",
    "        subscription_df.loc[subscription_df['currency'] == '1', 'subscription_amount'] /= 85.23\n",
    "        subscription_df = subscription_df.groupby('jobma_catcher_id').agg(\n",
    "            subscription_amount_in_dollars = ('subscription_amount', 'sum'),\n",
    "            number_of_subscriptions = ('subscription_amount', 'count'),\n",
    "        ).reset_index()\n",
    "        subscription_df['subscription_amount_in_dollars'] = subscription_df['subscription_amount_in_dollars'].round(3)\n",
    "        subscription_df.drop_duplicates(inplace=True)\n",
    "        return subscription_df\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4bbafa5-800b-4930-a617-c9b89d9a0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login_df\n",
    "\n",
    "def fetching_login_df(login_df):\n",
    "    print(\"Processing login DF\")\n",
    "\n",
    "    # Calculating Number of Gaps between last login and today\n",
    "    try:\n",
    "        login_df['jobma_last_login'] = pd.to_datetime(login_df['jobma_last_login'], errors='coerce')\n",
    "        login_df['activity_duration'] = (pd.Timestamp('today') - login_df['jobma_last_login']).dt.days\n",
    "        login_df['activity_duration'].fillna(370, inplace=True)\n",
    "        login_df['activity_duration'] = login_df['activity_duration'].astype(int)\n",
    "    \n",
    "        # Binning\n",
    "        bins = [-1,7,30,90,180,365,float('inf')]\n",
    "        labels = ['Less than 1 Week', '1-4 Weeks', '1-3 Months', '3-6 Months', '6-12 Months', 'More than 1 Year']\n",
    "        login_df['activity_duration'] = pd.cut(login_df['activity_duration'], bins=bins, labels=labels)\n",
    "        login_df = login_df[['jobma_catcher_id', 'activity_duration']]\n",
    "        return login_df\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef06aa61-60dc-4c90-a5e5-bf0a4d372a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_features(invitation_df, job_posting_df, kit_df):\n",
    "    print(\"Fetching features\")\n",
    "\n",
    "    try:\n",
    "        job_posting_df['job_posted'] = job_posting_df['jobma_catcher_id'].map(job_posting_df['jobma_catcher_id'].value_counts())\n",
    "        kit_df['number_of_kits'] = kit_df['jobma_catcher_id'].map(kit_df['jobma_catcher_id'].value_counts())\n",
    "    \n",
    "        invitation_df['number_of_invitations'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "        invitation_df = invitation_df[invitation_df['jobma_interview_mode'].isin(['1', '2'])].copy()\n",
    "        interview_counts = invitation_df.groupby(['jobma_catcher_id', 'jobma_interview_mode']).size().unstack(fill_value=0)\n",
    "        interview_counts = interview_counts.rename(columns={'1': 'number_of_recorded_interviews', '2': 'number_of_live_interviews'})\n",
    "        invitation_df = invitation_df.merge(interview_counts, on='jobma_catcher_id', how='left')\n",
    "    \n",
    "        #------\n",
    "        invitation_df = invitation_df[invitation_df['jobma_interview_status'] != '0']\n",
    "        invitation_df['interview_completed'] = invitation_df['jobma_catcher_id'].map(invitation_df['jobma_catcher_id'].value_counts())\n",
    "        invitation_df.drop(['jobma_interview_mode', 'jobma_interview_status'], axis=1, inplace=True)\n",
    "        #------\n",
    "        invitation_df = invitation_df.drop_duplicates()\n",
    "    \n",
    "        job_posting_df = job_posting_df[['jobma_catcher_id', 'job_posted']].drop_duplicates()\n",
    "        kit_df = kit_df[['jobma_catcher_id', 'number_of_kits']].drop_duplicates()\n",
    "    \n",
    "        return invitation_df, job_posting_df, kit_df\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "        return None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77520251-4d35-4303-9973-529bedad215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_df(catcher_df, wallet_df, subscription_df, invitation_df, job_posting_df, kit_df, login_df):\n",
    "    print(\"Merging DFs\")\n",
    "    try:\n",
    "        final_df = catcher_df.copy()\n",
    "    \n",
    "        # Left join each table one by one\n",
    "        try:\n",
    "            final_df = final_df.merge(login_df, on='jobma_catcher_id', how='left')\n",
    "            final_df = final_df.merge(wallet_df, on='jobma_catcher_id', how='left')\n",
    "            final_df = final_df.merge(subscription_df, on='jobma_catcher_id', how='left')\n",
    "            final_df = final_df.merge(invitation_df, on='jobma_catcher_id', how='left')\n",
    "            final_df = final_df.merge(job_posting_df, on='jobma_catcher_id', how='left')\n",
    "            final_df = final_df.merge(kit_df, on='jobma_catcher_id', how='left')\n",
    "            final_df.drop_duplicates(inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f'Error Merging DataFrames: {e}')\n",
    "            return None\n",
    "    \n",
    "        # For Total Sub\n",
    "        try:\n",
    "            sub_counts = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent').size()\n",
    "            final_df['total_sub'] = final_df['jobma_catcher_id'].map(sub_counts).fillna(0).astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Total Sub in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Total Sub in merging_df: {e}')        \n",
    "    \n",
    "        # For Kits\n",
    "        try:\n",
    "            sub_kits_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_kits'].sum()\n",
    "            kits_mapped = final_df['jobma_catcher_id'].map(sub_kits_sum).fillna(0)\n",
    "            final_df['number_of_kits'] = final_df['number_of_kits'].fillna(0) + kits_mapped\n",
    "            final_df['number_of_kits'] = final_df['number_of_kits'].astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Kits in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Kits in merging_df: {e}')\n",
    "        \n",
    "        # For Invitations\n",
    "        try:\n",
    "            sub_invitations_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_invitations'].sum()\n",
    "            invitations_mapped = final_df['jobma_catcher_id'].map(sub_invitations_sum).fillna(0)\n",
    "            final_df['number_of_invitations'] = final_df['number_of_invitations'].fillna(0) + invitations_mapped\n",
    "            final_df['number_of_invitations'] = final_df['number_of_invitations'].astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Invitations in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Invitations in merging_df: {e}')\n",
    "        \n",
    "        # For Job Posted\n",
    "        try:\n",
    "            sub_job_posted_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['job_posted'].sum()\n",
    "            job_posted_mapped = final_df['jobma_catcher_id'].map(sub_job_posted_sum).fillna(0)\n",
    "            final_df['job_posted'] = final_df['job_posted'].fillna(0) + job_posted_mapped\n",
    "            final_df['job_posted'] = final_df['job_posted'].astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Job Posted in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Job Posted in merging_df: {e}')\n",
    "    \n",
    "        # For Recorded Interviews\n",
    "        try:\n",
    "            sub_recorded_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_recorded_interviews'].sum()\n",
    "            recorded_mapped = final_df['jobma_catcher_id'].map(sub_recorded_sum).fillna(0)\n",
    "            final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].fillna(0) + recorded_mapped\n",
    "            final_df['number_of_recorded_interviews'] = final_df['number_of_recorded_interviews'].astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Recorded Interviews in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Recorded Interviews in merging_df: {e}')\n",
    "    \n",
    "        # For Live Interviews\n",
    "        try:\n",
    "            sub_live_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['number_of_live_interviews'].sum()\n",
    "            live_mapped = final_df['jobma_catcher_id'].map(sub_live_sum).fillna(0)\n",
    "            final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].fillna(0) + live_mapped\n",
    "            final_df['number_of_live_interviews'] = final_df['number_of_live_interviews'].astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Live Interviews in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Live Interviews in merging_df: {e}')\n",
    "    \n",
    "        # For Interview Completed\n",
    "        try:\n",
    "            sub_to_parent_sum = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['interview_completed'].sum()\n",
    "            final_df.loc[final_df['jobma_catcher_id'].isin(sub_to_parent_sum.index), 'interview_completed'] += final_df['jobma_catcher_id'].map(sub_to_parent_sum).fillna(0).astype(int)\n",
    "        except KeyError as e:\n",
    "            print(f'Error in Live Interviews in merging_df: {e}')\n",
    "        except TypeError as e:\n",
    "            print(f'Error of Int Conversion in Live Interviews in merging_df: {e}')\n",
    "        \n",
    "        # For Minimum Login Days\n",
    "        try:\n",
    "            login_order = {\n",
    "                'Less than 1 Week':0,\n",
    "                '1-4 Weeks':1,\n",
    "                '1-3 Months':2,\n",
    "                '3-6 Months':3,\n",
    "                '6-12 Months':4,\n",
    "                'More than 1 Year':5\n",
    "            }\n",
    "        \n",
    "            # For Login\n",
    "            final_df['activity_duration'] = final_df['activity_duration'].map(login_order).fillna(5).astype(int)\n",
    "            # It will calculate the minimum activity of subcatcher\n",
    "            sub_min_login = final_df[final_df['jobma_catcher_parent'] != 0].groupby('jobma_catcher_parent')['activity_duration'].min()\n",
    "            catcher_mask = final_df['jobma_catcher_id'].isin(sub_min_login.index)\n",
    "            final_df.loc[catcher_mask, 'activity_duration'] = np.minimum(\n",
    "                final_df.loc[catcher_mask, 'activity_duration'],\n",
    "                final_df.loc[catcher_mask, 'jobma_catcher_id'].map(sub_min_login)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'Error in Login in merging_df: {e}')\n",
    "    \n",
    "        df = final_df[final_df['jobma_catcher_parent'] == 0].copy()\n",
    "        df.drop(['jobma_catcher_parent'], axis=1, inplace=True)\n",
    "        \n",
    "        compare_df = df.copy()\n",
    "        df.drop('jobma_catcher_id', axis=1, inplace=True)\n",
    "    \n",
    "        print(f\"Final merged df shape is {df.shape}\")\n",
    "    \n",
    "        return df, compare_df\n",
    "    except Exception as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ee37a5-973e-497e-9ad1-c186d842937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(final_df):\n",
    "    final_df = final_df.copy()\n",
    "\n",
    "    # Step 1: Replace inf with NaN first\n",
    "    try: \n",
    "        final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "        # Step 2: Fill NaNs\n",
    "        fill_values = {\n",
    "            'subscription_status': 0,\n",
    "            'company_size': '1-25',\n",
    "            'activity_duration': 5,\n",
    "            'is_unlimited': 0,\n",
    "            'subscription_amount_in_dollars': 0,\n",
    "            'number_of_subscriptions': 0,\n",
    "            'number_of_invitations': 0,\n",
    "            'interview_completed': 0,\n",
    "            'number_of_recorded_interviews': 0,\n",
    "            'number_of_live_interviews': 0,\n",
    "            'job_posted': 0,\n",
    "            'number_of_kits': 0,\n",
    "            'total_sub': 0,\n",
    "        }\n",
    "        final_df.fillna(fill_values, inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(f'Key Not Found: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'Error during fillna step: {e}')\n",
    "\n",
    "    # Step 3: Explicitly cast to int for the appropriate columns\n",
    "    try:\n",
    "        int_columns = [\n",
    "            'subscription_status',\n",
    "            'activity_duration',\n",
    "            'is_unlimited',\n",
    "            'number_of_subscriptions',\n",
    "            'jobma_interview_status',\n",
    "            'number_of_invitations',\n",
    "            'interview_completed',\n",
    "            'number_of_recorded_interviews',\n",
    "            'number_of_live_interviews',\n",
    "            'job_posted',\n",
    "            'number_of_kits',\n",
    "            'total_sub',\n",
    "        ]\n",
    "        \n",
    "        for col in int_columns:\n",
    "            if col in final_df.columns:\n",
    "                final_df[col] = pd.to_numeric(final_df[col], errors='coerce').fillna(0).astype(int)\n",
    "    except TypeError as e:\n",
    "        print(f'Error in Int Conversion: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'Error during Int Conversion: {e}')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e7b3bc-8d31-4846-8d24-5a3611a2d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Encoding \n",
    "\n",
    "def ordinal_encoder(df):\n",
    "    try:\n",
    "        ordinal_col = ['company_size']\n",
    "        company_size_order = ['1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "    \n",
    "        ordinal = OrdinalEncoder(categories=[company_size_order])\n",
    "    \n",
    "        encoded = ordinal.fit_transform(df[ordinal_col].astype(str))\n",
    "    \n",
    "        encoded_df = pd.DataFrame(encoded, columns=[f' {col}_ord' for col in ordinal_col], index=df.index)\n",
    "    \n",
    "        df.drop(columns=ordinal_col, inplace=True)\n",
    "    \n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error in ordinal_encoder: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac9780c-b3ea-4197-8c19-a338111ae2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation \n",
    "\n",
    "def log_transform(df):\n",
    "    try:\n",
    "        log_cols = [\n",
    "            'subscription_amount_in_dollars',\n",
    "            'number_of_subscriptions',\n",
    "            'interview_completed',\n",
    "            'number_of_invitations',\n",
    "            'number_of_recorded_interviews',\n",
    "            'number_of_live_interviews',\n",
    "            'job_posted',\n",
    "            'number_of_kits',\n",
    "            'activity_duration',\n",
    "            'total_sub'\n",
    "        ]\n",
    "\n",
    "        if not log_cols:\n",
    "            raise Exception('Log Columns are Empty')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    df = df.copy()\n",
    "    for col in log_cols:\n",
    "        if col in df.columns:\n",
    "            # fill NaNs\n",
    "            df[col] = df[col].fillna(0)\n",
    "            # if a number is less than zero, turn it into zero\n",
    "            df[col] = df[col].clip(lower=0)\n",
    "            # safe log1p\n",
    "            df[col] = np.log1p(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce9113a-2412-4dc8-9fa6-9f0bde47fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    try:\n",
    "        return torch.tensor(x, dtype=torch.float32)\n",
    "    except TypeError as e:\n",
    "        print(f\"Error Converting Values to Tensors: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c60a9f7-f3f9-463b-ae85-65370ce5ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        engine = create_connection()\n",
    "        wallet_df,subscription_df,invitation_df,job_posting_df,kit_df, login_df = create_df(engine)\n",
    "        self.wallet_df = wallet_df\n",
    "        self.subscription_df = subscription_df\n",
    "        self.invitation_df = invitation_df\n",
    "        self.job_posting_df = job_posting_df\n",
    "        self.kit_df = kit_df\n",
    "        self.login_df = login_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, catcher_df):\n",
    "        catcher = fetching_catcher_df(catcher_df)\n",
    "        wallet = fetching_wallet_df(self.wallet_df)\n",
    "        subscription = fetching_subscription_df(self.subscription_df)\n",
    "        login = fetching_login_df(self.login_df)\n",
    "        invitation, job_posting, kit = fetching_features(\n",
    "            self.invitation_df,\n",
    "            self.job_posting_df,\n",
    "            self.kit_df,\n",
    "        )\n",
    "        final_df, compare_df = merging_df(catcher, wallet, subscription, invitation, job_posting, kit, login)\n",
    "        self.compare_df_ = fill_missing_values(compare_df)\n",
    "        self.compare_df_.to_csv(\"compare_df.csv\", index=False)\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb8075f4-1e34-41b7-8ccc-5fb2d9323ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = fill_missing_values(X)\n",
    "        X = ordinal_encoder(X)\n",
    "        X = log_transform(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e9382c5-a2e8-4451-af61-1c6d66c89ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)  # bottleneck\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6b317f7-5850-43fa-bc29-0dcbb195399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae91c800-27cd-4224-9a34-5a8d272f95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model():\n",
    "    print(\"Starting with model training\")\n",
    "\n",
    "    try:\n",
    "        # Pipelines\n",
    "        merge_pipeline = Pipeline([\n",
    "            ('merge', MergingTransformer())\n",
    "        ])\n",
    "    \n",
    "        # Step 2: Preprocess merged data\n",
    "        preprocess_pipeline = Pipeline([\n",
    "            ('preprocessing', PreprocessingTransformer()),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('to_tensor', FunctionTransformer(to_tensor))\n",
    "        ])\n",
    "    \n",
    "        full_pipeline = Pipeline([\n",
    "            ('merge_pipeline', merge_pipeline),\n",
    "            ('preprocess_pipeline', preprocess_pipeline)\n",
    "        ])\n",
    "    \n",
    "        X_tensor = full_pipeline.fit_transform(create_df(create_connection(),catcher_only=True))\n",
    "        \n",
    "        with open('full_pipeline.pkl', 'wb') as f:\n",
    "            pickle.dump(full_pipeline, f)\n",
    "        \n",
    "        X_df = pd.DataFrame(X_tensor)\n",
    "        X_data = CustomDataset(X_tensor)\n",
    "    \n",
    "        # BATCH_SIZE = 32\n",
    "        BATCH_SIZE = 16\n",
    "        dataloader = DataLoader(X_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "        # Initializing the Model \n",
    "        input_shape = X_df.shape[1]\n",
    "        model_1 = AutoEncoder(input_shape)\n",
    "    \n",
    "        # Important Parameters \n",
    "        learning_rate = 0.001\n",
    "        epochs = 100\n",
    "        # epochs = 50\n",
    "        patience = 10\n",
    "        delta = 1e-4\n",
    "        best_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        training_losses = []\n",
    "    \n",
    "        # Loss Function, Optimizers and LR Scheduler \n",
    "        loss_function = nn.SmoothL1Loss()\n",
    "        optimizer = torch.optim.AdamW(model_1.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model_1.train()\n",
    "            epoch_loss = 0\n",
    "    \n",
    "            for batch in dataloader:\n",
    "                encoded, decoded = model_1(batch)\n",
    "                loss = loss_function(decoded, batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                epoch_loss += loss.item()\n",
    "    \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            training_losses.append(avg_loss)\n",
    "            scheduler.step(avg_loss)\n",
    "    \n",
    "            print(f\"Epoch: {epoch+1}/{epochs} | Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "            # Early Stopping\n",
    "            if avg_loss < best_loss - delta:\n",
    "                best_loss = avg_loss\n",
    "                epochs_no_improve = 0\n",
    "    \n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "            # Print current learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print(f\"Learning Rate: {param_group['lr']:.6f}\")\n",
    "        print(\"Saving Model and Embeddings\")\n",
    "        torch.save(model_1.state_dict(),'model.pth')\n",
    "        encoder, _ = model_1(X_tensor)\n",
    "        latent_np = encoder.cpu().detach().numpy()\n",
    "    \n",
    "        with open('latent_np.pkl', 'wb') as embeddings_file:\n",
    "            pickle.dump(latent_np, embeddings_file)\n",
    "    \n",
    "        print(\"Model and Embeddings Saved\")\n",
    "    except Exception as e:\n",
    "        print(f'Error in Training: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7df1a6b2-0fc3-44fa-9ee4-fadb5fa07c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expecting catcher_id to fetch all the details of user\n",
    "\n",
    "def predict_using_catcher_id(catcher_id, n=5):\n",
    "\n",
    "    #For Minimum Login Days\n",
    "    login_order = {\n",
    "        'Less than 1 Week':0,\n",
    "        '1-4 Weeks':1,\n",
    "        '1-3 Months':2,\n",
    "        '3-6 Months':3,\n",
    "        '6-12 Months':4,\n",
    "        'More than 1 Year':5\n",
    "    }\n",
    "    \n",
    "    # Load the full pipeline and model\n",
    "    with open('full_pipeline.pkl', 'rb') as f:\n",
    "        full_pipeline = pickle.load(f)\n",
    "\n",
    "    # Extract Pipelines\n",
    "    preprocess_pipeline = full_pipeline.named_steps['preprocess_pipeline']\n",
    "    compare_df = pd.read_csv('compare_df.csv')\n",
    "\n",
    "    # Get the row corresponding to the input catcher_id\n",
    "    user_row = compare_df[compare_df['jobma_catcher_id'] == catcher_id]\n",
    "    print(user_row)\n",
    "\n",
    "    # Exception Handling\n",
    "    try:\n",
    "        if user_row.empty:\n",
    "            raise ValueError(f\"No data found for catcher_id: {catcher_id}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_input = user_row.drop(columns=['jobma_catcher_id']).iloc[0].to_dict()\n",
    "    user_df = pd.DataFrame([user_input])\n",
    "    transformed_input = preprocess_pipeline.transform(user_df)\n",
    "\n",
    "    # Load trained autoencoder model\n",
    "    input_shape = transformed_input.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "    model_1.load_state_dict(torch.load('model.pth'))\n",
    "    model_1.eval()\n",
    "\n",
    "    # Load latent embeddings\n",
    "    with open('latent_np.pkl', 'rb') as embeddings_file:\n",
    "        embeddings = pickle.load(embeddings_file)\n",
    "\n",
    "    # Generate user embedding\n",
    "    with torch.no_grad():\n",
    "        user_embedding, _ = model_1(torch.tensor(transformed_input, dtype=torch.float32))\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "\n",
    "        latent_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        latent_embeddings = F.normalize(latent_embeddings_tensor, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity and get top n+1 (to account for the input catcher_id)\n",
    "        similarities = F.cosine_similarity(user_embedding, latent_embeddings, dim=1)\n",
    "        top_indices = similarities.topk(n + 1).indices.cpu().numpy()\n",
    "\n",
    "    # Final Recommendation DataFrame\n",
    "    recommended = compare_df.iloc[top_indices].copy()\n",
    "\n",
    "    # Drop the row corresponding to the input catcher_id\n",
    "    recommended = recommended[recommended['jobma_catcher_id'] != catcher_id]\n",
    "\n",
    "    # Ensure exactly 'n' rows are returned\n",
    "    recommended = recommended.head(n)\n",
    "\n",
    "    # Add similarity score to the recommendations\n",
    "    recommended['similarity'] = similarities[top_indices[:len(recommended)]].cpu().numpy()[:len(recommended)]\n",
    "\n",
    "    # Replace encoded activity_duration with actual values\n",
    "    recommended['activity_duration'] = recommended['activity_duration'].replace({v: k for k, v in login_order.items()})\n",
    "\n",
    "    return recommended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce34447a-2228-4b66-b36e-8bbbbc8ea203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expecting all details from user_input\n",
    "\n",
    "def predict(user_input, n=5):\n",
    "\n",
    "    #For Minimum Login Days\n",
    "    login_order = {\n",
    "        'Less than 1 Week':0,\n",
    "        '1-4 Weeks':1,\n",
    "        '1-3 Months':2,\n",
    "        '3-6 Months':3,\n",
    "        '6-12 Months':4,\n",
    "        'More than 1 Year':5\n",
    "    }\n",
    "    \n",
    "    with open('full_pipeline.pkl', 'rb') as f:\n",
    "        full_pipeline = pickle.load(f)\n",
    "\n",
    "    # Extract pipelines\n",
    "    preprocess_pipeline = full_pipeline.named_steps['preprocess_pipeline']\n",
    "\n",
    "    # access compare_df using .csv file instead of fetching it from database\n",
    "    compare_df = pd.read_csv('compare_df.csv')\n",
    "\n",
    "    # Transform user input only with preprocessing pipeline\n",
    "    user_df = pd.DataFrame([user_input])\n",
    "    transformed_input = preprocess_pipeline.transform(user_df)\n",
    "    # print(f'Transformed User Input Type is: {type(transformed_input)}')\n",
    "\n",
    "    # Load trained model\n",
    "    input_shape = transformed_input.shape[1]\n",
    "    model_1 = AutoEncoder(input_shape)\n",
    "    model_1.load_state_dict(torch.load('model.pth'))\n",
    "    model_1.eval()\n",
    "\n",
    "    # Load latent embeddings\n",
    "    with open('latent_np.pkl', 'rb') as embeddings_file:\n",
    "        embeddings = pickle.load(embeddings_file)\n",
    "    # print(f'Embedding Type is {type(embeddings)}')\n",
    "    \n",
    "    # Generate user embedding\n",
    "    with torch.no_grad():\n",
    "        user_embedding, _ = model_1(transformed_input)\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "        # print(f'User Embedding Type is: {type(user_embedding)}')\n",
    "\n",
    "        latent_embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        latent_embeddings = F.normalize(latent_embeddings_tensor, p=2, dim=1)\n",
    "        # print(f'Latent Embedding Type is {type(latent_embeddings)}')\n",
    "\n",
    "        # Compute cosine similarity and get top-N\n",
    "        similarities = F.cosine_similarity(user_embedding, latent_embeddings, dim=1)\n",
    "        top_indices = similarities.topk(n).indices.cpu().numpy()\n",
    "    \n",
    "    # Final Recommendation DataFrame\n",
    "    recommended = compare_df.iloc[top_indices].copy()\n",
    "    \n",
    "    # To show the actual activity duration\n",
    "    recommended['activity_duration'] = recommended['activity_duration'].replace({v:k for k,v in login_order.items()})\n",
    "\n",
    "    # To show the Similarity Score\n",
    "    recommended['similarity'] = similarities[top_indices].cpu().numpy()\n",
    "    print(user_pref_good)\n",
    "    \n",
    "    return recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea0d4d99-f35d-4fb9-9d55-4e8186e994a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_good = {\n",
    "                 'subscription_status':0,\n",
    "                 'company_size':'101-500',\n",
    "                'activity_duration':4,\n",
    "                 'is_unlimited':0,\n",
    "                 'subscription_amount_in_dollars': 100.00,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':25,\n",
    "                'interview_completed':10,\n",
    "                'number_of_recorded_interviews':8,\n",
    "                'number_of_live_interviews':5,\n",
    "                'job_posted':4,\n",
    "                'number_of_kits':7,\n",
    "                'total_sub':1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1669beda-3ee5-4454-9c3d-5d60d087f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref_test = {\n",
    "                  'subscription_status':1,\n",
    "                'company_size':'1-25',\n",
    "                  'activity_duration':3,\n",
    "                 'is_unlimited':1,\n",
    "                 'subscription_amount_in_dollars': 125.0,\n",
    "                 'number_of_subscriptions':1,\n",
    "                 'number_of_invitations':18,\n",
    "                 'interview_completed':10,\n",
    "                  'number_of_recorded_interviews':523,\n",
    "                'number_of_live_interviews':1,\n",
    "                 'job_posted':1,\n",
    "                 'number_of_kits':1,\n",
    "                'total_sub':1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b760a2-d964-4379-aab3-29b169a708a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with model training\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Wallet DF\n",
      "Subscription DF\n",
      "Invitation DF\n",
      "Job posting DF\n",
      "kit DF\n",
      "Login DF\n",
      "creating connection with DB\n",
      "connection created successfully\n",
      "creating DFs=============\n",
      "Catcher DF is requested\n",
      "Processing catcher DF\n",
      "Processing wallet DF\n",
      "Processing subscription DF\n",
      "Processing login DF\n",
      "Fetching features\n",
      "Merging DFs\n",
      "Final merged df shape is (4464, 13)\n",
      "Epoch: 1/100 | Loss: 0.169485\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 2/100 | Loss: 0.087364\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 3/100 | Loss: 0.067519\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 4/100 | Loss: 0.058209\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 5/100 | Loss: 0.055294\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 6/100 | Loss: 0.052212\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 7/100 | Loss: 0.046695\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 8/100 | Loss: 0.042973\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 9/100 | Loss: 0.041529\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 10/100 | Loss: 0.038917\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 11/100 | Loss: 0.037752\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 12/100 | Loss: 0.036372\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 13/100 | Loss: 0.035335\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 14/100 | Loss: 0.032625\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 15/100 | Loss: 0.030610\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 16/100 | Loss: 0.028782\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 17/100 | Loss: 0.028690\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 18/100 | Loss: 0.027615\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 19/100 | Loss: 0.025288\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 20/100 | Loss: 0.026427\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 21/100 | Loss: 0.025055\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 22/100 | Loss: 0.025932\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 23/100 | Loss: 0.026181\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 24/100 | Loss: 0.024645\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 25/100 | Loss: 0.023701\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 26/100 | Loss: 0.023671\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 27/100 | Loss: 0.023360\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 28/100 | Loss: 0.023074\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 29/100 | Loss: 0.022435\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 30/100 | Loss: 0.023300\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 31/100 | Loss: 0.023351\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 32/100 | Loss: 0.021188\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 33/100 | Loss: 0.021058\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 34/100 | Loss: 0.020672\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 35/100 | Loss: 0.023625\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 36/100 | Loss: 0.021298\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 37/100 | Loss: 0.020524\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 38/100 | Loss: 0.019671\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 39/100 | Loss: 0.021032\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 40/100 | Loss: 0.019877\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 41/100 | Loss: 0.019363\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 42/100 | Loss: 0.020303\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 43/100 | Loss: 0.019240\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 44/100 | Loss: 0.018158\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 45/100 | Loss: 0.018589\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 46/100 | Loss: 0.019227\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 47/100 | Loss: 0.017177\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 48/100 | Loss: 0.017966\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 49/100 | Loss: 0.018045\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 50/100 | Loss: 0.018370\n",
      "Learning Rate: 0.001000\n",
      "Epoch: 51/100 | Loss: 0.017954\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 52/100 | Loss: 0.014086\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 53/100 | Loss: 0.013142\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 54/100 | Loss: 0.012878\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 55/100 | Loss: 0.013015\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 56/100 | Loss: 0.012292\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 57/100 | Loss: 0.013657\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 58/100 | Loss: 0.012620\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 59/100 | Loss: 0.011893\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 60/100 | Loss: 0.011949\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 61/100 | Loss: 0.012548\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 62/100 | Loss: 0.012633\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 63/100 | Loss: 0.011878\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 64/100 | Loss: 0.011967\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 65/100 | Loss: 0.011682\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 66/100 | Loss: 0.012500\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 67/100 | Loss: 0.011510\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 68/100 | Loss: 0.011349\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 69/100 | Loss: 0.012098\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 70/100 | Loss: 0.011882\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 71/100 | Loss: 0.011976\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 72/100 | Loss: 0.010827\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 73/100 | Loss: 0.011054\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 74/100 | Loss: 0.012005\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 75/100 | Loss: 0.010994\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 76/100 | Loss: 0.010556\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 77/100 | Loss: 0.011633\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 78/100 | Loss: 0.011622\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 79/100 | Loss: 0.010836\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 80/100 | Loss: 0.009956\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 81/100 | Loss: 0.010775\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 82/100 | Loss: 0.011062\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 83/100 | Loss: 0.010904\n",
      "Learning Rate: 0.000500\n",
      "Epoch: 84/100 | Loss: 0.010564\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 85/100 | Loss: 0.009851\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 86/100 | Loss: 0.008891\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 87/100 | Loss: 0.008858\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 88/100 | Loss: 0.008078\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 89/100 | Loss: 0.008529\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 90/100 | Loss: 0.008559\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 91/100 | Loss: 0.008177\n",
      "Learning Rate: 0.000250\n",
      "Epoch: 92/100 | Loss: 0.008348\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 93/100 | Loss: 0.008212\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 94/100 | Loss: 0.007816\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 95/100 | Loss: 0.007698\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 96/100 | Loss: 0.007847\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 97/100 | Loss: 0.007678\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 98/100 | Loss: 0.007527\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 99/100 | Loss: 0.007401\n",
      "Learning Rate: 0.000125\n",
      "Epoch: 100/100 | Loss: 0.007459\n",
      "Learning Rate: 0.000125\n",
      "Saving Model and Embeddings\n",
      "Model and Embeddings Saved\n"
     ]
    }
   ],
   "source": [
    "train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08acb270-79a5-487d-86ac-6f872b2756e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predict(user_pref_good, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 26\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(user_input, n)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Transform user input only with preprocessing pipeline\u001b[39;00m\n\u001b[0;32m     25\u001b[0m user_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([user_input])\n\u001b[1;32m---> 26\u001b[0m transformed_input \u001b[38;5;241m=\u001b[39m preprocess_pipeline\u001b[38;5;241m.\u001b[39mtransform(user_df)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print(f'Transformed User Input Type is: {type(transformed_input)}')\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m transformed_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:903\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    901\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[1;32m--> 903\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params[name]\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1046\u001b[0m     X,\n\u001b[0;32m   1047\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1048\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1049\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1050\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1051\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1052\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1053\u001b[0m )\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"
     ]
    }
   ],
   "source": [
    "predict(user_pref_good, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65ae01-8a57-4538-abf4-6f962de05c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(user_pref_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155043c5-8a56-4105-8bd3-b4b13482944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_using_catcher_id(6025, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a226d-46ae-473d-b469-13dec87f3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_using_catcher_id(6189, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
