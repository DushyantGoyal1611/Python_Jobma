{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14870ac9-f51a-447f-83ec-a1e393e36259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data Encoding and Scaling\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Natural Language Processing(NLP)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Word Embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470891ce-2d73-44ff-b584-ce4c0aa9140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27c1ff5-300e-4cab-83df-0aed765a1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad80787-b342-462a-8b26-91856bd35292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dushyant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dushyant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dushyant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2098b5c6-dbe5-4357-837a-ee5800c74470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06d0c6f-9f10-4469-a9f2-5fffc21d39c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' premium_plan, jobma_catcher_creation, \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Columns to recheck '''\n",
    "''' premium_plan, jobma_catcher_creation, \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5818144d-b25f-479a-ae6e-a4c8ccbd0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_collection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b77de0-0349-4975-b503-e61f000d796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['subscription_type_x',\n",
    "         'premium_storage_y',\n",
    "         'jobma_catcher_sub_accounts',\n",
    "         'jobma_job_company_profile',\n",
    "         'jobma_catcher_creation'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2484e38-0e54-4d11-8325-8484604d7ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jobma_catcher_id', 'org_type', 'jobma_catcher_industry',\n",
       "       'jobma_catcher_type', 'is_premium', 'jobma_catcher_sub_accounts',\n",
       "       'jobma_catcher_is_deleted', 'jobma_verified', 'subscription_status',\n",
       "       'interview_rate', 'live_interview_credit', 'pre_recorded_credit',\n",
       "       'credit_value', 'interview_cost_type', 'jobma_support_rtc',\n",
       "       'interview_question', 'video_recording_suppport',\n",
       "       'sing_up_canditate_after_apply', 'currency', 'company_size',\n",
       "       'credit_amount', 'wallet_amount', 'subscription_type_y', 'plan_type',\n",
       "       'is_unlimited', 'premium_storage', 'subscription_amount',\n",
       "       'number_of_subscriptions', 'number_of_invitations', 'job_posted',\n",
       "       'number_of_kits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7faa1e9-9c8c-452c-b104-dd3c9761ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['plan_type'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0082ff70-7f35-4d0f-ab7a-f43193a3fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_unlimited'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1574ca6-690b-4014-928a-08161cd5bc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6116, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be58ec2-92fd-4e4c-ba1b-961b2b5cb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This Function is to fill all missing values (if col is int then 0, if col is float then 0.0 and if col is object then 'Unkmown') '''\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.int64:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        elif df[col].dtype == np.float64:\n",
    "            df[col] = df[col].fillna(0.0)\n",
    "        elif df[col].dtype == object:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa76af15-5e26-42e1-b0eb-6dbd4b7d8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == np.int64:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    elif df[col].dtype == np.float64:\n",
    "        df[col].fillna(0.0, inplace=True)\n",
    "    elif df[col].dtype == object:\n",
    "        df[col].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9727f98-7a9f-4ceb-83c4-8e6361382a87",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "872ebbb1-ca1c-47d6-85ad-e1e7516b6ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['currency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7afaf812-8bb9-4179-8053-a8376c3ae86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "label_col = ['jobma_catcher_company', 'jobma_job_type', 'jobma_job_currency']\n",
    "\n",
    "def label_encoder(df):\n",
    "    df = df.copy()  \n",
    "\n",
    "    for col in label_col:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == object or df[col].dtype.name == 'category':\n",
    "                le_col = LabelEncoder()\n",
    "                df[col] = le_col.fit_transform(df[col].astype(str))\n",
    "                label_encoders[col] = le_col\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75fbee1b-0d3f-4137-bff6-aa32ac76da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['company_size'] = df['company_size'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7570fd-e4ef-4160-ba48-fd7337a86454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['company_size'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64227e6a-3a71-4880-b879-afa306184127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ordinal_encoder(df):\n",
    "#     df = df.copy()\n",
    "#     ordinal_col = ['company_size']\n",
    "#     ordinal_order = ['Unknown', '1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "\n",
    "#     ordinal = OrdinalEncoder(categories=[ordinal_order])\n",
    "#     ordinal_array = ordinal.fit_transform(df[ordinal_col])\n",
    "#     ordinal_df = pd.DataFrame(ordinal_array, columns=[f'{col}_ord' for col in ordinal_col], index=df.index)\n",
    "#     df = pd.concat([df, ordinal_df], axis=1)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81272a23-2327-4d8f-8005-e62f43a1fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encoder(df):\n",
    "    df = df.copy()\n",
    "    ordinal_col = ['company_size']\n",
    "    \n",
    "    # Define the order for encoding\n",
    "    ordinal_order = ['Unknown', '1-25', '26-100', '101-500', '500-1000', 'More than 1000']\n",
    "    \n",
    "    # Fit encoder\n",
    "    ordinal = OrdinalEncoder(categories=[ordinal_order])\n",
    "    encoded = ordinal.fit_transform(df[ordinal_col].astype(str))\n",
    "    \n",
    "    # Create DataFrame from encoded\n",
    "    encoded_df = pd.DataFrame(encoded, columns=[f'{col}_ord' for col in ordinal_col], index=df.index)\n",
    "    \n",
    "    # Drop original string column\n",
    "    df.drop(columns=ordinal_col, inplace=True)\n",
    "    \n",
    "    # Combine with rest\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55322461-e0af-49b6-b717-04a3b42eed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(df):\n",
    "    df = df.copy()\n",
    "    onehot_col = ['plan_type', 'is_unlimited', 'jobma_catcher_is_deleted']\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_array = ohe.fit_transform(df[onehot_col].astype(str))\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out(onehot_col), index=df.index)\n",
    "\n",
    "    # Drop original and concat encoded\n",
    "    df.drop(columns=onehot_col, inplace=True)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28364592-f0c9-4822-891c-ee21ddcde369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.astype(str).apply(lambda x: x.str.contains('Unknown', na=False)).any(axis=1)].apply(lambda row: row[row.str.contains('Unknown', na=False)].index.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f847d-1ba8-4f39-8b72-0a178525e828",
   "metadata": {},
   "source": [
    "**Creating Tags Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2dc890b-9986-424d-992e-0995b6ca9313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jobma_catcher_id', 'org_type', 'jobma_catcher_industry',\n",
       "       'jobma_catcher_type', 'is_premium', 'jobma_catcher_sub_accounts',\n",
       "       'jobma_catcher_is_deleted', 'jobma_verified', 'subscription_status',\n",
       "       'interview_rate', 'live_interview_credit', 'pre_recorded_credit',\n",
       "       'credit_value', 'interview_cost_type', 'jobma_support_rtc',\n",
       "       'interview_question', 'video_recording_suppport',\n",
       "       'sing_up_canditate_after_apply', 'currency', 'company_size',\n",
       "       'credit_amount', 'wallet_amount', 'subscription_type_y', 'plan_type',\n",
       "       'is_unlimited', 'premium_storage', 'subscription_amount',\n",
       "       'number_of_subscriptions', 'number_of_invitations', 'job_posted',\n",
       "       'number_of_kits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41500ee3-f995-4d94-b91d-3f3c6d580b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'jobma_catcher_company'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'jobma_catcher_company'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 2\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobma_catcher_company\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      3\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      4\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobma_catcher_indus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      5\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobma_job_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      6\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslug\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobma_job_functional_areas\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      8\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobma_job_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'jobma_catcher_company'"
     ]
    }
   ],
   "source": [
    "df['tags'] = (\n",
    "    df['jobma_catcher_company'].astype(str) + \" \"+\n",
    "    df['org_type'].astype(str) + \" \"+\n",
    "    df['jobma_catcher_indus'].astype(str) + \" \"+\n",
    "    df['jobma_job_title'].astype(str) + \" \"+\n",
    "    df['slug'].astype(str) + \" \"+\n",
    "    df['jobma_job_functional_areas'].astype(str) + \" \"+\n",
    "    df['jobma_job_keywords'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c7020-ab03-4936-9ff7-31d24edf87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['org_type',\n",
    "        'jobma_catcher_indus',\n",
    "        'jobma_job_title',\n",
    "        'slug',\n",
    "        'jobma_job_functional_areas',\n",
    "        'jobma_job_keywords'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96041280-920d-462e-bbb5-4b747f0bb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1,'tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0b8f8-6c42-474f-a71b-74ae56643500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d23a1-a13b-4403-8b0e-4bdbef928072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a76e6c-8a81-4eae-b095-4209ebcba058",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "**Note: Use Lemmatization for more accuracy**\n",
    "\n",
    "To normalize words and reduce them to their root forms, we will apply **stemming**. This helps in handling variations of words and improves text processing efficiency for machine learning models.  \n",
    "(e.g., \"running\" → \"run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479339ad-a1a1-4a02-9705-31b075db5263",
   "metadata": {},
   "source": [
    "**Currently using Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06779dec-0e0b-41bc-9f73-3066804d31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25c29b-005d-4f95-8075-18d36b1aa297",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2d4b7-0172-446c-983b-6a018a7fc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(text):\n",
    "    if isinstance(text, pd.Series) or isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    text = text.replace('/', '')\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b29968-9c72-4e47-909f-7cbaf93482e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    if isinstance(text,str):\n",
    "        words = text.split()\n",
    "        seen = set()\n",
    "        unique_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if word not in seen:\n",
    "                seen.add(word)\n",
    "                unique_words.append(word)\n",
    "    \n",
    "        return \" \".join(unique_words)        \n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ff167-c553-480b-bb4a-cfd53cf4de63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Word Embeddings\n",
    "**Note: Use Contextual Embeddings for More Accuracy**\n",
    "\n",
    "To represent words in a numerical format while preserving their meaning and relationships, we will apply **word embeddings**. This helps in capturing semantic similarities and improving machine learning model performance.\n",
    "(e.g., \"king\" → similar to \"queen\" but different from \"apple\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ef30e-96e1-4ff9-b520-2f0759e69a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df_or_series, text_col='tags', vector_size=50, window=7, min_count=1, workers=4):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    # Handle Series or ndarray input\n",
    "    if isinstance(df_or_series, (pd.Series, np.ndarray)):\n",
    "        df = pd.DataFrame(df_or_series, columns=[text_col])\n",
    "    elif isinstance(df_or_series, pd.DataFrame):\n",
    "        df = df_or_series.copy()\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas Series, DataFrame, or NumPy array\")\n",
    "\n",
    "    original_length = len(df)\n",
    "\n",
    "    # Clean text\n",
    "    cleaned = df[text_col].astype(str).str.lower().replace([\"nan\", \"none\"], \"\").str.strip()\n",
    "    cleaned = cleaned.replace(\"\", np.nan)\n",
    "\n",
    "    # Tokenization\n",
    "    valid_mask = cleaned.notna()\n",
    "    tokenized = [text.split() for text in cleaned[valid_mask]]\n",
    "    tokenized = [tokens for tokens in tokenized if tokens]\n",
    "\n",
    "    if not tokenized:\n",
    "        emb = np.zeros((original_length, vector_size))\n",
    "        emb_df = pd.DataFrame(emb, columns=[f'emb_{i}' for i in range(vector_size)])\n",
    "        return pd.concat([df.drop(columns=[text_col]).reset_index(drop=True), emb_df], axis=1)\n",
    "\n",
    "    # Train Word2Vec\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=tokenized,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers\n",
    "    )\n",
    "\n",
    "    # Convert text to average embedding\n",
    "    def text_to_embedding(text):\n",
    "        words = text.split()\n",
    "        vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "    embeddings = np.zeros((original_length, vector_size))\n",
    "    for idx in range(original_length):\n",
    "        text = cleaned.iloc[idx]\n",
    "        if pd.notna(text):\n",
    "            embeddings[idx] = text_to_embedding(text)\n",
    "\n",
    "    emb_df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(vector_size)])\n",
    "\n",
    "    # Final concat: drop original text col, keep all other original columns\n",
    "    result = pd.concat([df.drop(columns=[text_col]).reset_index(drop=True), emb_df], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d91d9-aa03-4707-9c10-a3bde9deca0f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e132d3-595c-49ac-9faa-f7b6d9b08381",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8e8cb-1c04-4403-8468-19dc113b11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_series(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        formatted_text = X.apply(lambda row: formatting(\" \".join(row.values.astype(str))), axis=1)\n",
    "    elif isinstance(X, pd.Series):\n",
    "        formatted_text = X.apply(formatting)\n",
    "    else:\n",
    "        formatted_text = pd.Series([formatting(text) for text in X])\n",
    "\n",
    "    return formatted_text.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c358f-958e-4591-a32e-1ee7d81bdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_series(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        cleaned_text = X.apply(lambda row: remove_duplicates(\" \".join(row.values.astype(str))), axis=1)\n",
    "    elif isinstance(X, pd.Series):\n",
    "        cleaned_text = X.apply(remove_duplicates)\n",
    "    else:\n",
    "        cleaned_text = pd.Series([remove_duplicates(text) for text in X])\n",
    "\n",
    "    return cleaned_text.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15ef21-0093-44cb-8d21-ea90ede36eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Pipeline for numerical columns '''\n",
    "\n",
    "# numerical_pipeline = Pipeline([\n",
    "#     ('fill_missing_values', FunctionTransformer(fill_missing_values, validate=False)),\n",
    "#     ('label_encoder', FunctionTransformer(label_encoder, validate=False)),\n",
    "#     ('scaler', StandardScaler()), # Needs attention to improve model\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94a5c5-101a-467f-9e4c-74b7d1159496",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pipeline for numerical columns '''\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('fill_missing_values', FunctionTransformer(fill_missing_values, validate=False)),\n",
    "    ('label_encoder', FunctionTransformer(label_encoder, validate=False)),\n",
    "    ('ordinal_encoder', FunctionTransformer(ordinal_encoder, validate=False)),\n",
    "    ('onehot_encoder', FunctionTransformer(onehot_encoder, validate=False)),\n",
    "    ('scaler', StandardScaler()), # Needs attention to improve model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb0330-1ed2-48b9-b032-68c62b139ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pipeline for text column (tags) '''\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('fill_missing_values', FunctionTransformer(fill_missing_values, validate=False)),\n",
    "    ('formatting', FunctionTransformer(formatting_series, validate=False)),\n",
    "    ('remove_duplicates', FunctionTransformer(remove_duplicates_series, validate=False)),\n",
    "    ('word_embedding', FunctionTransformer(word_embeddings, validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842c014-dd47-457e-b53b-f13ad26663ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Combining both numerical_pipeline and text_pipeline '''\n",
    "preprocessing_pipeline = ColumnTransformer(transformers=[\n",
    "    ('text', text_pipeline, ['tags']),\n",
    "    ('tabular', numerical_pipeline, [col for col in df.columns if col != 'tags'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a574b36-3f3f-4263-afae-83506a84fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualize the Pipeline '''\n",
    "\n",
    "set_config(display='diagram')\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8596659-f6fd-4d03-b830-ae90c5f40633",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b941a-f6b5-49f6-bdbf-939a4c1f1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee2412-a6e9-46a0-966f-a4e5a31c7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7bdfa-071b-4165-a79c-4ac799628937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Length of X_train: {len(X_train)}')\n",
    "print(f'Length of X_test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0876ae-932a-40fe-b4f6-02648bda2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train), type(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6b127-5827-4387-a9a0-c06b5831ed6f",
   "metadata": {},
   "source": [
    "# Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77714ffa-9e20-49d6-ab23-3512f0a0bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0aa5e8-35bc-4b0e-a207-93afdcf87bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Original X_train shape:\", X_train.shape)\n",
    "# print(\"Original X_train type:\", type(X_train))\n",
    "\n",
    "# num_out = numerical_pipeline.fit_transform(X_train.drop(columns=['tags']))\n",
    "# print(\"Numerical pipeline output shape:\", num_out.shape)\n",
    "# print(\"Numerical pipeline output type:\", type(num_out))\n",
    "\n",
    "# text_out = text_pipeline.fit_transform(X_train[['tags']])\n",
    "# print(\"Text pipeline output shape:\", text_out.shape)\n",
    "# print(\"Text pipeline output type:\", type(text_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb7744-07cb-4afb-832b-8ba6438de6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_transformed =  preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb47d9f-b9d4-478b-a10b-b62cf908d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340585e-5638-4321-903c-885a8a594d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train), type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a3d36-6901-45c5-82cf-b84a945fb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_transformed), type(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d027c47-90c8-407e-bc1d-3dc9e674abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e535a1a-a95b-4eed-8181-dcef8eba0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e211bc-ec80-47ea-9d2a-5a875f5a2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7469-deb9-4c97-b34d-e06435e4d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_transformed, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_transformed, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a866d0-6464-49e4-ac14-5e9c43111c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_tensor), type(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349f2d0-6d2f-44b0-9517-aecabeea0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_tensor), len(X_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997ddc1-6638-481e-9b5d-24ea19612cc3",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c28054-1ea2-46af-9912-181daa663844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ae036-0025-4a5b-b2e7-8a79f3765f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(X_train_tensor)\n",
    "test_data = CustomDataset(X_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39704b49-ef69-44e7-ade6-fd5543ed8e9b",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d8bd6-bba2-430b-b976-01e5108d995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad7d96-3ad4-491d-9439-0a12f915db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd87d70-6ef0-4cc2-9c94-c651ea153c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f35a7b-9003-40eb-81d8-33146ac0627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789fc65-5e02-4a2e-b49e-e525178f8d62",
   "metadata": {},
   "source": [
    "# Define a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957f086-9840-483a-9b46-8f2e648e5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client_Recommendation_Model(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        ''' Encoder (Compression) '''\n",
    "        # Shrinks job data into a small hidden representation (like a summary).\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "\n",
    "        ''' Decoder (Reconstruction) '''\n",
    "        # Tries to rebuild the original job data from that compressed version.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3285ff-7ef7-4542-870b-c4f8d5b0e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35a2b3-afdf-4d1f-ad46-2bfeadc0d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Initializing the model '''\n",
    "input_shape = X_df.shape[1]\n",
    "\n",
    "model_1 = Client_Recommendation_Model(input_shape)\n",
    "model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fe9be-d219-4ff8-a0a4-d2926f7fea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Important Parameters '''\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1773cd-8c0d-44c0-9fca-5144cb8bc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loss Function and Optimizer '''\n",
    "cosine_loss_function = nn.CosineEmbeddingLoss()\n",
    "mse_loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9d289-2678-4814-b785-e2ddd29d36aa",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b103d3-8e6e-4a6d-982f-74fb62544d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(model:torch.nn.Module,\n",
    "#                dataloader:torch.utils.data.DataLoader,\n",
    "#                mse_loss_function:torch.nn.Module,\n",
    "#                cosine_loss_function:torch.nn.Module,\n",
    "#                optimizer:torch.optim.Optimizer,\n",
    "#               device:torch.device):\n",
    "    \n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     epoch_total_loss = 0\n",
    "\n",
    "#     for batch_X in dataloader:\n",
    "#         batch_X = batch_X.to(device)\n",
    "#         encoded, decoded = model(batch_X)  # Forward pass\n",
    "\n",
    "#         ''' Compute Loss '''\n",
    "#         mse_loss = mse_loss_function(decoded, batch_X)  # Reconstruction Loss\n",
    "\n",
    "#         batch_size = encoded.shape[0]\n",
    "#         target_labels = torch.ones(batch_size, device=device)\n",
    "\n",
    "#         permuted_indices = torch.randperm(batch_size, device=device)\n",
    "#         encoded_shuffled = encoded[permuted_indices]\n",
    "\n",
    "#         cosine_loss = cosine_loss_function(encoded, encoded_shuffled, target_labels) # Similarity Loss\n",
    "#         total_loss = mse_loss + cosine_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_total_loss += total_loss.item()\n",
    "\n",
    "#     training_loss = epoch_total_loss / len(dataloader)\n",
    "#     return training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcad32-ac63-4599-b2c1-c38274ec1eca",
   "metadata": {},
   "source": [
    "# Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18445c-ffd6-4bce-9a46-c6fcc599129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_step(model:torch.nn.Module,\n",
    "#               dataloader:torch.utils.data.DataLoader,\n",
    "#               mse_loss_function:torch.nn.Module,\n",
    "#               cosine_loss_function:torch.nn.Module,\n",
    "#               device:torch.device\n",
    "#              ):\n",
    "\n",
    "#     epoch_total_loss = 0\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for batch_X in dataloader:\n",
    "#             batch_X = batch_X.to(device)\n",
    "#             encoded, decoded = model(batch_X)  # Forward pass\n",
    "\n",
    "#             ''' Compute Loss '''\n",
    "#             mse_loss = mse_loss_function(decoded, batch_X)  # Reconstruction Loss\n",
    "    \n",
    "#             batch_size = encoded.shape[0]\n",
    "#             target_labels = torch.ones(batch_size, device=device)\n",
    "\n",
    "#             # Compare each encoded job to another shuffled job\n",
    "#             permuted_indices = torch.randperm(batch_size, device=device)\n",
    "#             encoded_shuffled = encoded[permuted_indices]\n",
    "\n",
    "#             cosine_loss = cosine_loss_function(encoded, encoded_shuffled, target_labels)\n",
    "#             total_loss = mse_loss + cosine_loss\n",
    "            \n",
    "#             epoch_total_loss += total_loss.item()\n",
    "\n",
    "#         testing_loss = epoch_total_loss / len(dataloader)\n",
    "#         return testing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50ff45-8039-487e-a240-63f49bb5f6e2",
   "metadata": {},
   "source": [
    "# Combining Training and Testing Loop into evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43307d79-5a52-447c-be0f-b0c6b3921efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model:torch.nn.Module,\n",
    "#              train_dataloader:torch.utils.data.DataLoader,\n",
    "#              test_dataloader:torch.utils.data.DataLoader,\n",
    "#              mse_loss_function:torch.nn.Module,\n",
    "#              cosine_loss_function:torch.nn.Module,\n",
    "#              device:torch.device,\n",
    "#              epochs:int = 5,\n",
    "#              patience: int = 5\n",
    "#             ):\n",
    "    \n",
    "#     results = {'training_loss':[],\n",
    "#               'testing_loss':[]}\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     counter = 0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         training_loss = train_step(model=model,\n",
    "#                                    dataloader=train_dataloader,\n",
    "#                                    mse_loss_function=mse_loss_function,\n",
    "#                                    cosine_loss_function=cosine_loss_function,\n",
    "#                                    optimizer=optimizer,\n",
    "#                                    device=device\n",
    "#                                   )\n",
    "\n",
    "#         testing_loss = test_step(model=model,\n",
    "#                                    dataloader=test_dataloader,\n",
    "#                                    mse_loss_function=mse_loss_function,\n",
    "#                                    cosine_loss_function=cosine_loss_function,\n",
    "#                                  device=device\n",
    "#                                   )\n",
    "\n",
    "#         results['training_loss'].append(training_loss)\n",
    "#         results['testing_loss'].append(testing_loss)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{epochs} | Training Loss: {training_loss:.5f} | Testing Loss: {testing_loss:.5f}')\n",
    "\n",
    "#         if testing_loss < best_loss:\n",
    "#             best_loss = testing_loss\n",
    "#             counter = 0\n",
    "#         else:\n",
    "#             counter += 1\n",
    "#             if counter >= patience:\n",
    "#                 print(f\"Early Stopping triggered at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6fd82-4984-45a8-9154-65e6c2a92f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "# model_1_results = evaluate(model=model_1,\n",
    "#                         train_dataloader=train_dataloader,\n",
    "#                         test_dataloader=test_dataloader,\n",
    "#                         mse_loss_function=mse_loss_function,\n",
    "#                         cosine_loss_function=cosine_loss_function,\n",
    "#                         device=device,\n",
    "#                         epochs=epochs,\n",
    "#                         patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff44a0-ff87-4eea-b125-48c051509cc1",
   "metadata": {},
   "source": [
    "# Loss and Accuracy Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205188f-3405-4f80-b1b5-320c5fa0da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = range(len(model_1_results['training_loss']))\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(epochs, model_1_results['training_loss'], label='Training Loss')\n",
    "# plt.plot(epochs, model_1_results['testing_loss'], label='Testing Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training VS Testing Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc5185-4a26-47fc-8fa3-e368ff96c251",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d10622-74c1-4224-8508-62748a580fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, decoder = model_1(X_train_tensor)\n",
    "\n",
    "latent_np = decoder.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806851f-1c09-45a5-b8b9-0152ff405b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffcacf-e50f-4912-868b-9471c7c5ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(latent_np)\n",
    "    \n",
    "    wcss.append(kmeans.inertia_)  # Inertia = WCSS\n",
    "    silhouette_scores.append(silhouette_score(latent_np, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7124d7-3534-42e9-86dc-297fd93bcb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9258b85-d221-4dd3-a0c0-d71cc9b7d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhbg fcdxfgcjbhnkml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72ff5c-66ae-4acf-8fb8-434038755354",
   "metadata": {},
   "source": [
    "# Recommending 5 Top Clients (Based on Loyalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede22b92-f92a-4d4e-859d-23fd6dd745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bf554-686f-4fb9-b0a0-d2994bfc8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pref = {\n",
    "    'Age' : 22\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d5103-64e4-490b-bfc5-5d976e27e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pref_df = pd.DataFrame([company_pref])\n",
    "type(company_pref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf5d9e-fba2-4e01-b9dc-55be7f2ef5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pref_transformed = preprocessing_pipeline.transform(company_pref_df)\n",
    "type(company_pref_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b7843-07b0-418e-a8bd-e3898e8bfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pref_tensor = torch.tensor(company_pref_transformed, torch.float32)\n",
    "type(company_pref_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3827ec6-b739-4714-bc2c-145e6ada8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_pref_tensor.shape, X_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fc6a1-2795-4122-a5b3-f2fd6a8e575e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
